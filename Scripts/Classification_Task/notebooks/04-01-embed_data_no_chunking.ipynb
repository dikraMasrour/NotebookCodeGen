{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using codeBERT for the classification task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n",
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cobeBERT base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1342,  0.3599,  0.0361,  ..., -0.2329, -0.3161,  0.3294],\n",
       "         [-0.7013,  0.1173,  0.0651,  ..., -0.3564, -0.2514,  0.2654],\n",
       "         [-0.3371,  0.1115,  0.4299,  ..., -0.2361, -0.1156,  0.8037],\n",
       "         ...,\n",
       "         [-0.4057,  0.1638,  0.4813,  ..., -0.1657, -0.2869,  0.7310],\n",
       "         [-0.3968,  0.4617,  0.5130,  ..., -0.3096, -0.6014,  0.4400],\n",
       "         [-0.1354,  0.3618,  0.0367,  ..., -0.2342, -0.3183,  0.3317]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_tokens = tokenizer.tokenize(\"returns maximum value\")\n",
    "code_tokens = tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "context_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "context_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test codeBERT for md only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1342,  0.3342,  0.0396,  ..., -0.2193, -0.3251,  0.3286],\n",
       "         [-0.2917,  0.4377,  0.1195,  ..., -0.3086, -0.5924,  0.1050],\n",
       "         [ 0.0023,  0.2300, -0.0107,  ...,  0.0430, -0.4059,  0.0055],\n",
       "         [-0.1335,  0.3338,  0.0403,  ..., -0.2191, -0.3248,  0.3279]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_tokens = tokenizer.tokenize(\"returns category of a notebook\")\n",
    "\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "context_embeddings_md = model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "context_embeddings_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test codeBERT for code only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1264,  0.3699,  0.0476,  ..., -0.1938, -0.2802,  0.3096],\n",
       "         [-0.5152,  0.4615,  0.2745,  ..., -0.4479, -0.3483,  0.1919],\n",
       "         [-0.1253,  0.3687,  0.0483,  ..., -0.1938, -0.2787,  0.3072]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokens = tokenizer.tokenize(\n",
    "    \"\"\"for i in data.index:\n",
    "    if data.loc[i]['subcategory'] == sub:\n",
    "        categ = data.loc[i]['category']\n",
    "    \"\"\")\n",
    "\n",
    "tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "context_embeddings_code = model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "context_embeddings_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings.shape\n",
    "# 1 = number of batches\n",
    "# 23 = number of tokens\n",
    "# 768 = number of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing embedding aggregation using averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tensor into np array\n",
    "tensor_np = context_embeddings_md.cpu().detach().numpy()\n",
    "# average of embeddings of the tokens in the sequence\n",
    "avg_md = np.mean(tensor_np[0], axis=0)\n",
    "len(avg_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tensor into np array\n",
    "tensor_np = context_embeddings_code.cpu().detach().numpy()\n",
    "# average of embeddings of the tokens in the sequence\n",
    "avg_code = np.mean(tensor_np[0], axis=0)\n",
    "len(avg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the code and md sequences are semantically similar using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352593421936035"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = 1 - cosine(avg_md, avg_code)\n",
    "cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test CodeBERT for mask prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011970043182373047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 501201999,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1361d602686c4e079acecdf091239434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "mlm_model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "mlm_tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7230142951011658, 'token': 6595, 'token_str': ' import', 'sequence': ' importmatplotlib.pyplot as plt'}, {'score': 0.1273956149816513, 'token': 4, 'token_str': '.', 'sequence': '.matplotlib.pyplot as plt'}, {'score': 0.03073594532907009, 'token': 1215, 'token_str': '_', 'sequence': '_matplotlib.pyplot as plt'}, {'score': 0.023831013590097427, 'token': 479, 'token_str': '.', 'sequence': '.matplotlib.pyplot as plt'}, {'score': 0.010915211401879787, 'token': 18134, 'token_str': ' _', 'sequence': ' _matplotlib.pyplot as plt'}]\n"
     ]
    }
   ],
   "source": [
    "CODE = \"<mask> matplotlib.pyplot as plt\"\n",
    "mlm_model_name = \"microsoft/codebert-base-mlm\"\n",
    "fill_mask = pipeline('fill-mask', model=mlm_model, tokenizer=mlm_tokenizer)\n",
    "\n",
    "outputs = fill_mask(CODE)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test PLBART for code-to-text & text-to-code translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plbarttokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n",
    "plbart_model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'average representation of embeddings in a sequence of tokens'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_python_phrase = \"\"\"def average_embeddings(embedding_tensor):\n",
    "\n",
    "    avg_rep = np.empty(1)\n",
    "    if embedding_tensor is not None:\n",
    "        if type(embedding_tensor) == torch.Tensor:\n",
    "            # convert tensor into np array\n",
    "            tensor_np = embedding_tensor.cpu().detach().numpy()\n",
    "            # average of embeddings of the tokens in the sequence\n",
    "            avg_rep = np.mean(tensor_np[0], axis=0)\n",
    "        else: \n",
    "            tensor_np = embedding_tensor\n",
    "            # average of embeddings of the tokens in the sequence\n",
    "            avg_rep = np.mean(tensor_np, axis=0)\n",
    "        # return average representation of a sequence of tokens\n",
    "        return avg_rep\n",
    "\"\"\"\n",
    "inputs = plbarttokenizer(example_python_phrase, return_tensors=\"pt\")\n",
    "translated_tokens = plbart_model.generate(**inputs, decoder_start_token_id=plbarttokenizer.lang_code_to_id[\"en_XX\"])\n",
    "plbarttokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note to self: PLBART gives pretty good results!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and test T5 for text to text tranformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this https://huggingface.co/docs/transformers/model_doc/t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EMBEDDING REAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset containing clean text (code and md cells for each notebook and their category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'text_dataset.pkl', 'rb') as f:\n",
    "    text_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell-type</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344966</th>\n",
       "      <td>code</td>\n",
       "      <td>POI_data = gpd.read_file(\"../input/geospatial...</td>\n",
       "      <td>your-first-map.ipynb</td>\n",
       "      <td>reinforcement learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344967</th>\n",
       "      <td>markdown</td>\n",
       "      <td>next we create a map from all four geodatafram...</td>\n",
       "      <td>your-first-map.ipynb</td>\n",
       "      <td>reinforcement learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344968</th>\n",
       "      <td>code</td>\n",
       "      <td>ax = counties.plot(figsize=(10,10), color='no...</td>\n",
       "      <td>your-first-map.ipynb</td>\n",
       "      <td>reinforcement learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344969</th>\n",
       "      <td>markdown</td>\n",
       "      <td>it looks like the northeastern part of the sta...</td>\n",
       "      <td>your-first-map.ipynb</td>\n",
       "      <td>reinforcement learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344970</th>\n",
       "      <td>markdown</td>\n",
       "      <td>have questions or comments visit the course di...</td>\n",
       "      <td>your-first-map.ipynb</td>\n",
       "      <td>reinforcement learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cell-type                                             source  \\\n",
       "344966      code   POI_data = gpd.read_file(\"../input/geospatial...   \n",
       "344967  markdown  next we create a map from all four geodatafram...   \n",
       "344968      code   ax = counties.plot(figsize=(10,10), color='no...   \n",
       "344969  markdown  it looks like the northeastern part of the sta...   \n",
       "344970  markdown  have questions or comments visit the course di...   \n",
       "\n",
       "                       title                     tag  \n",
       "344966  your-first-map.ipynb  reinforcement learning  \n",
       "344967  your-first-map.ipynb  reinforcement learning  \n",
       "344968  your-first-map.ipynb  reinforcement learning  \n",
       "344969  your-first-map.ipynb  reinforcement learning  \n",
       "344970  your-first-map.ipynb  reinforcement learning  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332354, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding each cell's tokens and averaging them to get a representation for the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_single_row(text, row):\n",
    "\n",
    "    '''\n",
    "    gets a text string (code or md) and the row index\n",
    "    tests for sequences longer than 510 not to exceed the max_len limit of BERT model\n",
    "    tokenizes and returns embeddings tensor\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nl_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # we choose max_len to be 510 as the tokenizer then adds 2 special tokens <s> and </s>\n",
    "    if len(nl_tokens) > 510: \n",
    "        nl_tokens = nl_tokens[:510]\n",
    "\n",
    "    tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]\n",
    "\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    row_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "\n",
    "    if row_embeddings is not None:\n",
    "        return row_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embeddings(embedding_tensor):\n",
    "\n",
    "    '''\n",
    "    gets the returned embeddings tensor of a sequence and averages it \n",
    "    to return one 768 dim vector for each row in the dataframe\n",
    "\n",
    "    '''\n",
    "\n",
    "    avg_rep = np.empty(1)\n",
    "    if embedding_tensor is not None:\n",
    "        if type(embedding_tensor) == torch.Tensor:\n",
    "            # convert tensor into np array\n",
    "            tensor_np = embedding_tensor.cpu().detach().numpy()\n",
    "            # average of embeddings of the tokens in the sequence\n",
    "            avg_rep = np.mean(tensor_np[0], axis=0)\n",
    "        else: \n",
    "            tensor_np = embedding_tensor\n",
    "            # average of embeddings of the tokens in the sequence\n",
    "            avg_rep = np.mean(tensor_np, axis=0)\n",
    "        # return average representation of a sequence of tokens\n",
    "        return avg_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by chunks of the dataset for time optimization reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_avg_embedding = []\n",
    "for row in text_data.index[:166177]:\n",
    "    embedding = embed_single_row(text_data.loc[row, 'source'], row)\n",
    "    row_avg_embedding.append((average_embeddings(embedding),  text_data.loc[row, 'title'], text_data.loc[row, 'tag']))\n",
    "    print(row)\n",
    "    \n",
    "# build intermediary dataframe of averaged row vectors, titles and tags \n",
    "interm_df01 = pd.DataFrame(row_avg_embedding, columns = ['row_vector', 'title', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_avg_embedding = []\n",
    "for row in text_data.index[166177:(166177+100000)]:\n",
    "    embedding = embed_single_row(text_data.loc[row, 'source'], row)\n",
    "    row_avg_embedding.append((average_embeddings(embedding),  text_data.loc[row, 'title'], text_data.loc[row, 'tag']))\n",
    "    print(row)\n",
    "# build intermediary dataframe of averaged row vectors, titles and tags \n",
    "interm_df02 = pd.DataFrame(row_avg_embedding, columns = ['row_vector', 'title', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_avg_embedding = []\n",
    "for row in text_data.index[266177:]:\n",
    "    embedding = embed_single_row(text_data.loc[row, 'source'], row)\n",
    "    row_avg_embedding.append((average_embeddings(embedding),  text_data.loc[row, 'title'], text_data.loc[row, 'tag']))\n",
    "    print(row)\n",
    "# build intermediary dataframe of averaged row vectors, titles and tags \n",
    "interm_df03 = pd.DataFrame(row_avg_embedding, columns = ['row_vector', 'title', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorized chunks to pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_df01.to_pickle(DATA_PATH + 'interm_df_0_166176.pkl')\n",
    "interm_df02.to_pickle(DATA_PATH + 'interm_df_166177_266177.pkl')\n",
    "interm_df03.to_pickle(DATA_PATH + 'interm_df_266177_332354.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct our final dataset by grouping by notebook titles and averaging the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_vector</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.15601629, 0.07173749, 0.52512264, 0.035489...</td>\n",
       "      <td>0-9-try-better-parameters-better-score.ipynb</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.003272749, 0.27969736, 0.29870665, 0.12947...</td>\n",
       "      <td>0-9-try-better-parameters-better-score.ipynb</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.14450021, 0.05994332, -0.022545334, -0.018...</td>\n",
       "      <td>0-9-try-better-parameters-better-score.ipynb</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.025118893, 0.24624567, 0.13547924, 0.313845...</td>\n",
       "      <td>0-9-try-better-parameters-better-score.ipynb</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.48143202, 0.40252736, 0.4532451, -0.054363...</td>\n",
       "      <td>0-9-try-better-parameters-better-score.ipynb</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          row_vector  \\\n",
       "0  [-0.15601629, 0.07173749, 0.52512264, 0.035489...   \n",
       "1  [-0.003272749, 0.27969736, 0.29870665, 0.12947...   \n",
       "2  [-0.14450021, 0.05994332, -0.022545334, -0.018...   \n",
       "3  [0.025118893, 0.24624567, 0.13547924, 0.313845...   \n",
       "4  [-0.48143202, 0.40252736, 0.4532451, -0.054363...   \n",
       "\n",
       "                                          title         tag  \n",
       "0  0-9-try-better-parameters-better-score.ipynb  regression  \n",
       "1  0-9-try-better-parameters-better-score.ipynb  regression  \n",
       "2  0-9-try-better-parameters-better-score.ipynb  regression  \n",
       "3  0-9-try-better-parameters-better-score.ipynb  regression  \n",
       "4  0-9-try-better-parameters-better-score.ipynb  regression  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickles_dir = Path(DATA_PATH + 'pickles\\\\').glob('*')\n",
    "vect_data = pd.DataFrame()\n",
    "\n",
    "vect_data = pd.concat([pd.read_pickle(DATA_PATH + 'pickles\\\\' + filename.name) for filename in pickles_dir])\n",
    "vect_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332354, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_vector    0\n",
       "title         0\n",
       "tag           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_avg(df):\n",
    "    grp_avg_embedding = []\n",
    "\n",
    "    grp_by_title = df.groupby('title').groups\n",
    "    grp_titles = list(grp_by_title.keys())\n",
    "    for grp_title in grp_titles:\n",
    "        # get indices range for each title\n",
    "        grp_indices = grp_by_title[grp_title]\n",
    "        # get row embeddings for each notebook \n",
    "        grp_vecs = df.loc[grp_indices, 'row_vector']\n",
    "        # get group tag\n",
    "        grp_tag = df.loc[grp_indices[0], 'tag']\n",
    "        # call average method\n",
    "        grp_avg_embedding.append((list(average_embeddings(grp_vecs)), grp_tag)) \n",
    "        \n",
    "    vect_data = pd.DataFrame(grp_avg_embedding, columns = ['notebook_vector', 'tag'])\n",
    "    return vect_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_vector</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.34790626, 0.29865852, 0.30619365, 0.055174...</td>\n",
       "      <td>computer vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.3819649, 0.31716973, 0.33679396, 0.0810977...</td>\n",
       "      <td>clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.33451593, 0.2831545, 0.29143128, 0.0433948...</td>\n",
       "      <td>computer vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.27561176, 0.26736438, 0.2665188, 0.0765375...</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.24364452, 0.28803557, 0.2663721, 0.1090390...</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     notebook_vector              tag\n",
       "0  [-0.34790626, 0.29865852, 0.30619365, 0.055174...  computer vision\n",
       "1  [-0.3819649, 0.31716973, 0.33679396, 0.0810977...       clustering\n",
       "2  [-0.33451593, 0.2831545, 0.29143128, 0.0433948...  computer vision\n",
       "3  [-0.27561176, 0.26736438, 0.2665188, 0.0765375...              nlp\n",
       "4  [-0.24364452, 0.28803557, 0.2663721, 0.1090390...   classification"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data_final = group_and_avg(vect_data)\n",
    "vect_data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6260, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_data_final.to_csv(DATA_PATH + 'vect_data_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5093ef3c9eba5a2350b58945b72c1f122b8b5551b9ae00db57837a16e1175c4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: https://towardsdatascience.com/how-to-do-average-and-max-word-embedding-for-long-sentences-f3531e99d998*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(max_len, sent):\n",
    "    \"\"\"because the embedding function is trained on dim 512, so we have to limit the size of the sentences using max_len so the final chunked sentences wont exceed length 512\n",
    "    Args:\n",
    "        max_len (int): maximum number of tokens for each chunk\n",
    "        sent (str): input sentence\n",
    "    Returns:\n",
    "        sent_chunk (List(str)): list of chunked sentences\n",
    "    \"\"\"\n",
    "    tokenized_text = sent.lower().split(' ')\n",
    "    bert_tokenized_text = tokenizer.tokenize(sent)\n",
    "\n",
    "    tokens = [tokenizer.cls_token]+bert_tokenized_text+[tokenizer.sep_token]\n",
    "\n",
    "    if len(tokens) > max_len:\n",
    "        # using list comprehension to divide the sequence\n",
    "        final = [\n",
    "            tokenized_text[i * max_len : (i + 1) * max_len]\n",
    "            for i in range((len(tokenized_text) + max_len - 1) // max_len)\n",
    "        ]\n",
    "\n",
    "        # join back to sentences for each of the chunks\n",
    "        sent_chunk = []\n",
    "        for item in final:\n",
    "            try:\n",
    "                # make sure the len(items) > 1 or else some of the embeddings will appear as len 1 instead of 768.\n",
    "                assert len(item) > 1\n",
    "            except Exception as e:\n",
    "                print(item, e)\n",
    "            sent_chunk.append(\" \".join(item))\n",
    "        return sent_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit : https://www.geeksforgeeks.org/insert-row-at-given-position-in-pandas-dataframe/\n",
    "# Function to insert row in the dataframe\n",
    "def insert_row_to_df(row_number, df, row_value):\n",
    "    # Slice the upper half of the dataframe\n",
    "    df1 = df[0:row_number]\n",
    "  \n",
    "    # Store the result of lower half of the dataframe\n",
    "    df2 = df[row_number:]\n",
    "  \n",
    "    # Insert the row in the upper half dataframe\n",
    "    df1.loc[row_number]=row_value\n",
    "    \n",
    "    # Concat the two dataframes\n",
    "    df_result = pd.concat([df1, df2])\n",
    "  \n",
    "    # Reassign the index labels\n",
    "    df_result.index = [*range(df_result.shape[0])]\n",
    "\n",
    "\n",
    "    # Return the updated dataframe\n",
    "    # print(df_result.loc[row_number])\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_df(chunks, df, row):\n",
    "    # df.drop(index=row, inplace=True)\n",
    "    for chunk in chunks:\n",
    "        temp_tuple =  df.loc[row, 'cell-type'], chunk, df.loc[row, 'title'], df.loc[row, 'tag']\n",
    "        # print(temp_tuple)\n",
    "        \n",
    "        row_value = list(temp_tuple)\n",
    "        print(row_value)\n",
    "\n",
    "        # test = insert_row_to_df(row, df, row_value).copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49ab456e11cde720218fba409a85456f40f210cf294d5c8f56d5f4fb69af5c6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

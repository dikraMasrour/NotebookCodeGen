{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This kernel experiments with various data augmentation techniques for facial keypoint detection."},{"metadata":{},"cell_type":"markdown","source":"## Importing Necessary Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Making necessary imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom math import sin, cos, pi\nimport cv2\nfrom tqdm.notebook import tqdm\n\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmentation & Hyperparameter Settings\n#### Experiment with various augmentation choices"},{"metadata":{"trusted":true},"cell_type":"code","source":"horizontal_flip = False\nrotation_augmentation = True\nbrightness_augmentation = True\nshift_augmentation = True\nrandom_noise_augmentation = True\n\ninclude_unclean_data = True    # Whether to include samples with missing keypoint values. Note that the missing values would however be filled using Pandas' 'ffill' later.\nsample_image_index = 20    # Index of sample train image used for visualizing various augmentations\n\nrotation_angles = [12]    # Rotation angle in degrees (includes both clockwise & anti-clockwise rotations)\npixel_shifts = [12]    # Horizontal & vertical shift amount in pixels (includes shift from all 4 corners)\n\nNUM_EPOCHS = 80\nBATCH_SIZE = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting files to working directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Contents of input/facial-keypoints-detection directory: \")\n!ls ../input/facial-keypoints-detection/\n\nprint(\"\\nExtracting .zip dataset files to working directory ...\")\n!unzip -u ../input/facial-keypoints-detection/test.zip\n!unzip -u ../input/facial-keypoints-detection/training.zip\n\nprint(\"\\nCurrent working directory:\")\n!pwd\nprint(\"\\nContents of working directory:\")\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading inputs to a Pandas DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_file = 'training.csv'\ntest_file = 'test.csv'\nidlookup_file = '../input/facial-keypoints-detection/IdLookupTable.csv'\ntrain_data = pd.read_csv(train_file)\ntest_data = pd.read_csv(test_file)\nidlookup_data = pd.read_csv(idlookup_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Function to plot facial keypoints with images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idlookup_data.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for any images with missing pixel values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Length of train data: {}\".format(len(train_data)))\nprint(\"Number of Images with missing pixel values: {}\".format(len(train_data) - int(train_data.Image.apply(lambda x: len(x.split())).value_counts().values)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find columns having Null values and their counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can observe that approx. 68% of data is missing for several keypoints"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n\nclean_train_data = train_data.dropna()\nprint(\"clean_train_data shape: {}\".format(np.shape(clean_train_data)))\n\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\nunclean_train_data = train_data.fillna(method = 'ffill')\nprint(\"unclean_train_data shape: {}\\n\".format(np.shape(unclean_train_data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate data into clean & unclean subsets"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef load_images(image_data):\n    images = []\n    for idx, sample in image_data.iterrows():\n        image = np.array(sample['Image'].split(' '), dtype=int)\n        image = np.reshape(image, (96,96,1))\n        images.append(image)\n    images = np.array(images)/255.\n    return images\n\ndef load_keypoints(keypoint_data):\n    keypoint_data = keypoint_data.drop('Image',axis = 1)\n    keypoint_features = []\n    for idx, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n    keypoint_features = np.array(keypoint_features, dtype = 'float')\n    return keypoint_features\n\nclean_train_images = load_images(clean_train_data)\nprint(\"Shape of clean_train_images: {}\".format(np.shape(clean_train_images)))\nclean_train_keypoints = load_keypoints(clean_train_data)\nprint(\"Shape of clean_train_keypoints: {}\".format(np.shape(clean_train_keypoints)))\ntest_images = load_images(test_data)\nprint(\"Shape of test_images: {}\".format(np.shape(test_images)))\n\ntrain_images = clean_train_images\ntrain_keypoints = clean_train_keypoints\nfig, axis = plt.subplots()\nplot_sample(clean_train_images[sample_image_index], clean_train_keypoints[sample_image_index], axis, \"Sample image & keypoints\")\n\nif include_unclean_data:\n    unclean_train_images = load_images(unclean_train_data)\n    print(\"Shape of unclean_train_images: {}\".format(np.shape(unclean_train_images)))\n    unclean_train_keypoints = load_keypoints(unclean_train_data)\n    print(\"Shape of unclean_train_keypoints: {}\\n\".format(np.shape(unclean_train_keypoints)))\n    train_images = np.concatenate((train_images, unclean_train_images))\n    train_keypoints = np.concatenate((train_keypoints, unclean_train_keypoints))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing Horizontal Flipping for Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def left_right_flip(images, keypoints):\n    flipped_keypoints = []\n    flipped_images = np.flip(images, axis=2)   # Flip column-wise (axis=2)\n    for idx, sample_keypoints in enumerate(keypoints):\n        flipped_keypoints.append([96.-coor if idx%2==0 else coor for idx,coor in enumerate(sample_keypoints)])    # Subtract only X co-ordinates of keypoints from 96 for horizontal flipping\n    return flipped_images, flipped_keypoints\n\nif horizontal_flip:\n    flipped_train_images, flipped_train_keypoints = left_right_flip(clean_train_images, clean_train_keypoints)\n    print(\"Shape of flipped_train_images: {}\".format(np.shape(flipped_train_images)))\n    print(\"Shape of flipped_train_keypoints: {}\".format(np.shape(flipped_train_keypoints)))\n    train_images = np.concatenate((train_images, flipped_train_images))\n    train_keypoints = np.concatenate((train_keypoints, flipped_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(flipped_train_images[sample_image_index], flipped_train_keypoints[sample_image_index], axis, \"Horizontally Flipped\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing Rotation Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotate_augmentation(images, keypoints):\n    rotated_images = []\n    rotated_keypoints = []\n    print(\"Augmenting for angles (in degrees): \")\n    for angle in rotation_angles:    # Rotation augmentation for a list of angle values\n        for angle in [angle,-angle]:\n            print(f'{angle}', end='  ')\n            M = cv2.getRotationMatrix2D((48,48), angle, 1.0)\n            angle_rad = -angle*pi/180.     # Obtain angle in radians from angle in degrees (notice negative sign for change in clockwise vs anti-clockwise directions from conventional rotation to cv2's image rotation)\n            # For train_images\n            for image in images:\n                rotated_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                rotated_images.append(rotated_image)\n            # For train_keypoints\n            for keypoint in keypoints:\n                rotated_keypoint = keypoint - 48.    # Subtract the middle value of the image dimension\n                for idx in range(0,len(rotated_keypoint),2):\n                    # https://in.mathworks.com/matlabcentral/answers/93554-how-can-i-rotate-a-set-of-points-in-a-plane-by-a-certain-angle-about-an-arbitrary-point\n                    rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n                    rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n                rotated_keypoint += 48.   # Add the earlier subtracted value\n                rotated_keypoints.append(rotated_keypoint)\n            \n    return np.reshape(rotated_images,(-1,96,96,1)), rotated_keypoints\n\nif rotation_augmentation:\n    rotated_train_images, rotated_train_keypoints = rotate_augmentation(clean_train_images, clean_train_keypoints)\n    print(\"\\nShape of rotated_train_images: {}\".format(np.shape(rotated_train_images)))\n    print(\"Shape of rotated_train_keypoints: {}\\n\".format(np.shape(rotated_train_keypoints)))\n    train_images = np.concatenate((train_images, rotated_train_images))\n    train_keypoints = np.concatenate((train_keypoints, rotated_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(rotated_train_images[sample_image_index], rotated_train_keypoints[sample_image_index], axis, \"Rotation Augmentation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing Brightness Alteration for Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def alter_brightness(images, keypoints):\n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)    # Increased brightness by a factor of 1.2 & clip any values outside the range of [-1,1]\n    dec_brightness_images = np.clip(images*0.6, 0.0, 1.0)    # Decreased brightness by a factor of 0.6 & clip any values outside the range of [-1,1]\n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    return altered_brightness_images, np.concatenate((keypoints, keypoints))\n\nif brightness_augmentation:\n    altered_brightness_train_images, altered_brightness_train_keypoints = alter_brightness(clean_train_images, clean_train_keypoints)\n    print(f\"Shape of altered_brightness_train_images: {np.shape(altered_brightness_train_images)}\")\n    print(f\"Shape of altered_brightness_train_keypoints: {np.shape(altered_brightness_train_keypoints)}\")\n    train_images = np.concatenate((train_images, altered_brightness_train_images))\n    train_keypoints = np.concatenate((train_keypoints, altered_brightness_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[sample_image_index], altered_brightness_train_keypoints[sample_image_index], axis, \"Increased Brightness\") \n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[len(altered_brightness_train_images)//2+sample_image_index], altered_brightness_train_keypoints[len(altered_brightness_train_images)//2+sample_image_index], axis, \"Decreased Brightness\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing Horizontal & Vertical shift"},{"metadata":{"trusted":true},"cell_type":"code","source":"def shift_images(images, keypoints):\n    shifted_images = []\n    shifted_keypoints = []\n    for shift in pixel_shifts:    # Augmenting over several pixel shift values\n        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n            for image, keypoint in zip(images, keypoints):\n                shifted_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])\n                if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<96.0):\n                    shifted_images.append(shifted_image.reshape(96,96,1))\n                    shifted_keypoints.append(shifted_keypoint)\n    shifted_keypoints = np.clip(shifted_keypoints,0.0,96.0)\n    return shifted_images, shifted_keypoints\n\nif shift_augmentation:\n    shifted_train_images, shifted_train_keypoints = shift_images(clean_train_images, clean_train_keypoints)\n    print(f\"Shape of shifted_train_images: {np.shape(shifted_train_images)}\")\n    print(f\"Shape of shifted_train_keypoints: {np.shape(shifted_train_keypoints)}\")\n    train_images = np.concatenate((train_images, shifted_train_images))\n    train_keypoints = np.concatenate((train_keypoints, shifted_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(shifted_train_images[sample_image_index], shifted_train_keypoints[sample_image_index], axis, \"Shift Augmentation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding Random Noise for Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(images):\n    noisy_images = []\n    for image in images:\n        noisy_image = cv2.add(image, 0.008*np.random.randn(96,96,1))    # Adding random normal noise to the input image & clip the resulting noisy image between [-1,1]\n        noisy_images.append(noisy_image.reshape(96,96,1))\n    return noisy_images\n\nif random_noise_augmentation:\n    noisy_train_images = add_noise(clean_train_images)\n    print(f\"Shape of noisy_train_images: {np.shape(noisy_train_images)}\")\n    train_images = np.concatenate((train_images, noisy_train_images))\n    train_keypoints = np.concatenate((train_keypoints, clean_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(noisy_train_images[sample_image_index], clean_train_keypoints[sample_image_index], axis, \"Random Noise Augmentation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Train images & corresponding Keypoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of final train_images: {}\".format(np.shape(train_images)))\nprint(\"Shape of final train_keypoints: {}\".format(np.shape(train_keypoints)))\n\nprint(\"\\n Clean Train Data: \")\nfig = plt.figure(figsize=(20,8))\nfor i in range(10):\n    axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n    plot_sample(clean_train_images[i], clean_train_keypoints[i], axis, \"\")\nplt.show()\n\nif include_unclean_data:\n    print(\"Unclean Train Data: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(unclean_train_images[i], unclean_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif horizontal_flip:\n    print(\"Horizontal Flip Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif rotation_augmentation:\n    print(\"Rotation Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(rotated_train_images[i], rotated_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif brightness_augmentation:\n    print(\"Brightness Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif shift_augmentation:\n    print(\"Shift Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif random_noise_augmentation:\n    print(\"Random Noise Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(noisy_train_images[i], clean_train_keypoints[i], axis, \"\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n# Input dimensions: (None, 96, 96, 1)\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 96, 96, 32)\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Input dimensions: (None, 48, 48, 32)\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 48, 48, 64)\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Input dimensions: (None, 24, 24, 64)\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 24, 24, 96)\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Input dimensions: (None, 12, 12, 96)\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 12, 12, 128)\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Input dimensions: (None, 6, 6, 128)\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 6, 6, 256)\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Input dimensions: (None, 3, 3, 256)\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n# Input dimensions: (None, 3, 3, 512)\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n# Input dimensions: (None, 3, 3, 512)\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Load a pre-trained model (if present)\nif os.path.exists('../input/data-augmentation-for-facial-keypoint-detection/best_model.hdf5'):\n    model = load_model('../input/data-augmentation-for-facial-keypoint-detection/best_model.hdf5')\n\n# Define necessary callbacks\ncheckpointer = ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'acc'])\n\n# Train the model\nhistory = model.fit(train_images, train_keypoints, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_split=0.05, callbacks=[checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for mean_absolute_error\ntry:\n    plt.plot(history.history['mae'])\n    plt.plot(history.history['val_mae'])\n    plt.title('Mean Absolute Error vs Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper right')\n    plt.show()\n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Accuracy vs Epoch')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss vs Epoch')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\nexcept:\n    print(\"One of the metrics used for plotting graphs is missing! See 'model.compile()'s `metrics` argument.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model on full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Modify ModelCheckpoint callback to save model with best train mae to disk (instead of best validation mae)\ncheckpointer = ModelCheckpoint(filepath = 'best_model.hdf5', monitor='mae', verbose=1, save_best_only=True, mode='min')\nmodel.fit(train_images, train_keypoints, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[checkpointer])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n \nmodel = load_model('best_model.hdf5')\ntest_preds = model.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Test Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,16))\nfor i in range(20):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(test_images[i], test_preds[i], axis, \"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = list(idlookup_data['FeatureName'])\nimage_ids = list(idlookup_data['ImageId']-1)\nrow_ids = list(idlookup_data['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(test_preds[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If this notebook helped, please upvote it :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
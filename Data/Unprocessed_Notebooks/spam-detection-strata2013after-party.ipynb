{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## SPAM detection task\nThe data contains 100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. The task is to make a spam detector. \ntrain.csv - contains 600 emails x 100 features for use training model(s)\ntrain_labels.csv - contains labels for the 600 training emails (1 = spam, 0 = normal)\ntest.csv - contains 4000 emails x 100 features. Need to detect the spam on them.\n\nPredictions can be continuous numbers or 0/1 labels. No header is necessary. Submissions are judged on area under the ROC curve. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Will import libraries\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize as sp\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import linear_model, model_selection, metrics, tree, ensemble ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading data\ndata = pd.read_csv('../input/just-the-basics-the-after-party/train.csv')\ndataT = pd.read_csv('../input/just-the-basics-the-after-party/test.csv')\ny = pd.read_csv('../input/just-the-basics-the-after-party/train_labels.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the dataset has no headers, let's name the columns for further incrimination. \ncolums = list((range(0,100)))\ndata.columns = [colums]\ndataT.columns = [colums]\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And let's fill in the missing values with the median\nfor i in colums:\n    data[i,].fillna(data[i,].median(), inplace = True)\n\nfor i in colums:\n    dataT[i,].fillna(dataT[i,].median(), inplace = True)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's bring y to the required shape\n\ny_train = np.ravel(y)\nprint(y.shape,type(y), y_train.shape, type(y_train))\n\n#Data is full, no need delete outliers (NEED MORE Explanations)\nX_train = data\nX_test = dataT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n### Will tune hyperparameters using GridSearchCV. For scoring will use area under the ROC curve: 'roc_auc'."},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For penalty will use Lasso 'l1'. Tune 'C' parameter\nparam_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n\nestimator = linear_model.LogisticRegression(solver='liblinear', penalty = 'l1', random_state = 1)\noptimizerL = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerL.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerL.best_score_)\nprint('param_opt', optimizerL.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RidgeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 2, 5]}\n\nestimator = linear_model.RidgeClassifier( random_state = 1)\noptimizerR = GridSearchCV(estimator, param_grid,  scoring = 'roc_auc',cv = 3)                    \noptimizerR.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerR.best_score_)\nprint('param_opt', optimizerR.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier\nWe should have a loose stopping criterion and then use pruning to remove branches that contribute to overfitting. But pruning is a tradeoff between accuracy and generalizability, so our train scores might lower but the difference between train and test scores will also get lower.  This is what we need.  (details - https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_class = ensemble.RandomForestClassifier(random_state = 1)\ntrain_scores, test_scores = model_selection.validation_curve(rf_class, X_train, y_train, 'max_depth', list(range(1, 11)), cv=3, scoring='roc_auc')\nprint('max_depth=', list(range(1, 10)))\nprint(train_scores.mean(axis = 1))\nprint(test_scores.mean(axis = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get the same difference between train and test scores on by  max_depth=4-9\nAnd we have the bigger score ROC AUC by max_depth=4"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': list(range(20, 100, 5)), 'min_weight_fraction_leaf': [0.001,  0.005, 0.01, 0.05, 0.1, 0.5] } \n\nestimator = ensemble.RandomForestClassifier(max_depth=4, random_state = 1)\noptimizerRF = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerRF.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerRF.best_score_)\nprint('param_opt', optimizerRF.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth': list(range(1, 7)), 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1, 1.5], 'n_estimators': list(range(10, 100, 5)) }\nestimator = xgb.XGBClassifier( random_state = 1, min_child_weight=3)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': list(range(10, 100, 5)), 'min_child_weight': list(range(1, 10)) }\nestimator = xgb.XGBClassifier( max_depth = 3, random_state = 1, learning_rate=0.1)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will use the highest value ROC AUC model - RandomForestClassifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writting answers\n\nans=optimizerRF.predict(X_test)\n\nf=open(\"/kaggle/working/answers.csv\", \"w\")\nf.write(str(ans))\nf.close()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
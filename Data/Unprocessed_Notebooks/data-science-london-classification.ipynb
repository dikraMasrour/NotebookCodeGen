{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Introduction:**\nThis tutorial is for beginners learning the concept of `Scikit-Learn library` which is a high level framework designed for supervised and unsupervised machine learning algorithms, built on top of NumPy and SciPy libraries, each responsible for lower-level data science tasks.\n\n### Author: Abdelwahed Ashraf \n\n### Linkedin: [Link](https://www.linkedin.com/in/abdelwahed-ashraf-090523169/)\n\n### Kaggle: [Link](https://www.kaggle.com/abdelwahed43)\n\n**The sequence of steps are as follows:**\n\n* Check for missing values in the dataset\n* Pre-process data by splitting into Train-Test sets\n* Models Classification\n* Feature Scaling by standardizing and normalizing your data\n    > * Learn its effect by improved accuracy\n* Reduce the dimension of your data using PCA\n    > * Learn its effect by improved accuracy\n* Applying Gaussian Mixture and Grid Search\n    > * Learn its effect by improved accuracy\n* Fit our best model \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outline\nThe following sections are included in this notebook:\n\n### A. [Load and Parse Data](#section-one)\n\n### B. [Exploratory Data Analysis (EDA)](#section-two)\n   1. [Missing Data](#section-two-a)    \n   \n    \n\n      \n        \n### C. [Fit and Evaluate the Model](#section-four)\n   1. [Cross-Validation](#section-four-a)\n       \n       * Naive Bayes classifier\n       * KNN or k-Nearest Neighbors  \n       * Random Forest \n       * Logistic Regression             \n       * Support Vector Machines  \n       * Decision Tree\n       * XGBOOST Classifier \n       * AdaBoosting Classifier\n       * GradientBoostingClassifier \n       * HistGradientBoostingClassifier\n       * Principal Component Analysis (PCA) \n       * Gaussian Mixture \n       * Grid Search\n      \n   \n   2. [Model Stacking](#section-four-c)\n    \n### D. [Predict Test Dataset and Submit](#section-five)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# A. Load and Parse Data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames.\nWe also combine these datasets to run certain operations on both datasets together.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# read csv (comma separated value) into data\ntrain = pd.read_csv('../input/data-science-london-scikit-learn/train.csv', header=None)\ntrainLabel = pd.read_csv('../input/data-science-london-scikit-learn/trainLabels.csv', header=None)\ntest = pd.read_csv('../input/data-science-london-scikit-learn/test.csv', header=None)\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# B. Exploratory Data Analysis (EDA)\nThe purpose of EDA is to get familiar with our data, but not so familiar that we begin making assumptions about the model fit! In Kaggle competitions it can be tempting to overfit the training data in hopes of a lower test score, but this often doesn't bode well for real world applications. Typically, it's best to let the data speak for itself and allow the model the flexibility to find correlations between the target and features. Afterall, the models in todays age are very robust. \n\n### Do Preprocessing Later!\nThis is really more of a personal opinion. I find it hard to keep track of data processing done in cells throughout an EDA section. Typically, I prefer to do all the preprocessing in a single code block or even better in a pipeline. This way I know the preprocessing is being applied the same way to the train, validation, and test datasets. I use EDA as a way to identify the preprocessing steps that need to take place and potential feature engineering opportunities. \n\nRemember, it's best to do preprocessing in a pipeline!!!\n\nIn this section I will explore the following common issues:\n1. Missing Data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### B.1 check shape of training and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data Description')\nprint('The shape of our training set: ',train.shape[0], 'rows ', 'and', train.shape[1]  , 'columns'  )\nprint('The shape of our testing set: ',trainLabel.shape[0], 'rows', 'and', trainLabel.shape[1], 'column')\nprint('The shape of our testing set: ',test.shape[0], 'rows', 'and', test.shape[1], 'columns')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B.2 Analyze by describing data\nPandas also helps describe the datasets answering following questions early in our project.\n\n\n### Which features are available in the dataset?\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview the data from head\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B.3 Descriptive statistics\n***split training data into numeric and categorical data***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data train into Numeric and Categorocal\nnumeric = train.select_dtypes(exclude='object')\ncategorical = train.select_dtypes(include='object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Which features are categorical?\n * These values classify the samples into sets of similar samples.\n * Within categorical features are the values nominal, ordinal, ratio, or interval based? \n * Among other things this helps us select the appropriate plots for visualization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Which features are numerical?\n* Which features are numerical? These values change from sample to sample.\n* Within numerical features are the values discrete, continuous, or timeseries based?\n* Among other things this helps us select the appropriate plots for visualization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What are the data types for various features?\n\n* Helping us during converting goal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Quantitative data:\n*  discrete, continuous\n* are measures of values or counts and are expressed as numbers.\n\n* Quantitative data are data about numeric variables (e.g. how many; how much; or how often).\n\n### Qualitative data:\n*  ordinal , nominal\n* are measures of 'types' and may be represented by a name, symbol, or a number code.\n\n* Qualitative data are data about categorical variables (e.g. what type).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Note All Features are Numerical (Quantitative data[](http://))","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.info()\nprint('_'*50)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two-a\"></a>\n### B.4 Missing Data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n* Correcting by dropping features\n* Correcting by fill features\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data in Traing examples\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data in Traing Label (target label) examples\ntotal = trainLabel.isnull().sum().sort_values(ascending=False)\npercent = (trainLabel.isnull().sum()/trainLabel.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data in Test examples\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(trainLabel.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C. Model, predict and solve\n- Grid Search\n- Naive Bayes classifier\n- KNN or k-Nearest Neighbors\n- Random Forrest\n- Logistic Regression\n- Support Vector Machines\n- Decision Tree\n- XGBOOST Classifier\n- AdaBoosting Classifier\n- GradientBoostingClassifier\n- HistGradientBoostingClassifier\n- Principal Component Analysis (PCA)\n- Gaussian Mixture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\n\n# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n#SVM\nfrom sklearn.svm import SVC\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n#XGBOOST\nfrom xgboost import XGBClassifier\n#AdaBoosting Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n#GradientBoosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#HistGradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,GridSearchCV\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler ,Normalizer , MinMaxScaler, RobustScaler \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We further split the training set in to a train and test set to validate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(train,trainLabel,test_size=0.30, random_state=101)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Coming to the modeling part. We first scale the data using standard scaler.\nWe use grid search with stratified kfold validation for 9 algorithms.\nWe get the scores from the cross validation for all these models and run a prediction on the test data from our train_test_split.\nFor stacking we get the accuracy based on fitting the train and test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n#sc =StandardScaler()\n\nX_train= X_train\n\nX_train_1= train.values\n\nX_test= X_test\n\nX_submit= test.values\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Traing Models with Feature Scaling\n\n### **Feature Scaling**\n\nTwo approaches are shown below:\n1. The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\n2. The **normalizer** scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n#sc =StandardScaler()\n\nsc =StandardScaler()\n#sc =Normalizer()\n\nX_train= sc.fit_transform(X_train)\n\nX_train_1= sc.transform(train.values)\n\nX_test= sc.transform(X_test)\n\nX_submit= sc.transform(test.values)\n\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN gave maximum accuracy using Feature Scaling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Principal Component Analysis**\n\nPCA helps us to identify patterns in data based on the correlation between features. Used to reduce number of variables in your data by extracting important one from a large pool. Thus, it reduces the dimension of your data with the aim of retaining as much information as possible.\n\nHere we will use a straightforward PCA, asking it to preserve 85% of the variance in the projected data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"from sklearn.decomposition import PCA\n\npca = PCA(0.85, whiten=True)\npca_train_data = pca.fit_transform(train)\nprint(pca_train_data.shape,'\\n')\n\nexplained_variance = pca.explained_variance_ratio_ \nprint(explained_variance)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Introducing another concept now i.e. **K-Fold Cross-validation**, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n\nCross-Validation can be used to evaluate performance of a model by handling the variance problem of the result set.\n\nIn this approach, the data used for training and testing are non-overlapping. To implement, first separate your data set into two subsets. One subset you use for training and other for testing. Now, do the exercise again by swapping the data sets. Report the average test result. This is call 2-fold cross validation. \n\nSimilarly if you divide your entire data set in to five sub sets and perform the exercise ten times and report the average test result then that would be 10-fold cross validation (which is what we'll be doing now).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\ntrain=pd.DataFrame(pca_train_data)\ntest=pd.DataFrame(pca.transform(test))\n\nprint(train.shape)\nprint(trainLabel.shape)\nprint(test.shape)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"X_train,X_test,y_train,y_test = train_test_split(train,trainLabel,test_size=0.30, random_state=101)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n\n\nX_train= X_train\n\nX_train_1= train.values\n\nX_test= X_test\n\nX_submit = test.values\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[99],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Applying Gaussian Mixture and Grid Search to improve the accuracy**\n\nWe select the above three algorithms **(KNN, Random Forest and SVM)** which  gave maximum accuracy for further analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.svm import SVC\n\nX = np.r_[train,test]\nprint('X shape :',X.shape)\nprint('\\n')\n\n# USING THE GAUSSIAN MIXTURE MODEL \n\n#The Bayesian information criterion (BIC) can be used to select the number of components in a Gaussian Mixture in an efficient way. \n#In theory, it recovers the true number of components only in the asymptotic regime\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\n\n#The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: \n# spherical, diagonal, tied or full covariance.\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.aic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(train)\ngmm_test = best_gmm.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **predict_proba** method will take in new data points and predict the responsibilities for each Gaussian. In other words, the probability that this data point came from each distribution.\n\n\n\n**Now Applying Grid Search Algorithm:** \n\nTo identify the best algorithm and best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(gmm_train,trainLabel,test_size=0.30, random_state=101)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n\n\nX_train= X_train\n\n\nX_train_1= pd.DataFrame(gmm_train).values\n\nX_test= X_test\n\nX_submit =  pd.DataFrame(gmm_test).values\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,6,7,8,9,10],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[10, 50, 100, 200,400],\"max_depth\":[3, 10, 20, 40],\"random_state\":[99],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":[0.05,0.0001,0.01,0.001],\"C\":[0.1, 1, 10, 100, 1000]},),\\\n      \n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_1.shape)\nprint(trainLabel.shape)\nprint(X_submit.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting our model\nstack_list[3].fit(X_train_1,trainLabel)\ny_submit = stack_list[3].predict(X_submit)\n\ny_submit= pd.DataFrame(y_submit)\ny_submit.index +=1\n\n\n\n# FRAMING OUR SOLUTION\ny_submit.columns = ['Solution']\ny_submit['Id'] = np.arange(1,y_submit.shape[0]+1)\ny_submit = y_submit[['Id', 'Solution']]\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_submit.shape)\nprint(y_submit.head(8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exporting the data to submit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_submit.to_csv('Submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
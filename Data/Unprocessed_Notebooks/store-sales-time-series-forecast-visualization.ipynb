{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Store Sales. Time Series Forecast & Visualization\nHello everyone! In this my new notebook we are going to explore Time Series (dataset: Store Sales). \n\n\n### References\nFor this notebook I would like to say thank you some authors for their notebooks that have inspired me to write own notebook:\n\n1. [KASHISH RASTOGI. üìùStore Sales Analysis‚è≥ Time Serie](https://www.kaggle.com/kashishrastogi/store-sales-forecasting)\n\n2. [HOWOO JANG. First kaggle notebook. Following TS tutorial](https://www.kaggle.com/howoojang/first-kaggle-notebook-following-ts-tutorial)\n\n3. [EKREM BAYAR. Store Sales TS Forecasting - A Comprehensive Guide](https://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide)\n\n4. [Ryan Holbrook. Time Series](https://www.kaggle.com/learn/time-series)","metadata":{}},{"cell_type":"markdown","source":"# 1. Import libraries","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.linear_model import LinearRegression\nfrom pandas import date_range\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\n\n# Model 1 (trend)\nfrom pyearth import Earth\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\n\n# Model 2\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.multioutput import RegressorChain\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-05T12:45:19.560099Z","iopub.execute_input":"2021-11-05T12:45:19.560678Z","iopub.status.idle":"2021-11-05T12:45:19.569514Z","shell.execute_reply.started":"2021-11-05T12:45:19.560633Z","shell.execute_reply":"2021-11-05T12:45:19.568874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# switch off the warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:45:19.571051Z","iopub.execute_input":"2021-11-05T12:45:19.573324Z","iopub.status.idle":"2021-11-05T12:45:19.584595Z","shell.execute_reply.started":"2021-11-05T12:45:19.573287Z","shell.execute_reply":"2021-11-05T12:45:19.583919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Read data","metadata":{}},{"cell_type":"code","source":"df_holidays = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv', header = 0)\ndf_oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv', header = 0)\ndf_stores = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv', header = 0)\ndf_trans = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv', header = 0)\n\ndf_train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv', header = 0)\ndf_test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv', header = 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:45:19.585707Z","iopub.execute_input":"2021-11-05T12:45:19.585928Z","iopub.status.idle":"2021-11-05T12:45:21.806418Z","shell.execute_reply.started":"2021-11-05T12:45:19.585899Z","shell.execute_reply":"2021-11-05T12:45:21.805674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we need to convert all ***'date'*** columns to datetime Pandas format:","metadata":{}},{"cell_type":"code","source":"df_holidays['date'] = pd.to_datetime(df_holidays['date'], format = \"%Y-%m-%d\")\ndf_oil['date'] = pd.to_datetime(df_oil['date'], format = \"%Y-%m-%d\")\ndf_trans['date'] = pd.to_datetime(df_trans['date'], format = \"%Y-%m-%d\")\ndf_train['date'] = pd.to_datetime(df_train['date'], format = \"%Y-%m-%d\")\ndf_test['date'] = pd.to_datetime(df_test['date'], format = \"%Y-%m-%d\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:45:21.80836Z","iopub.execute_input":"2021-11-05T12:45:21.80864Z","iopub.status.idle":"2021-11-05T12:45:22.297551Z","shell.execute_reply.started":"2021-11-05T12:45:21.808602Z","shell.execute_reply":"2021-11-05T12:45:22.296804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can look and check our different dataframes:","metadata":{}},{"cell_type":"markdown","source":"Here we can see **df_holidays**:","metadata":{}},{"cell_type":"code","source":"df_holidays.head(10) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.299005Z","iopub.execute_input":"2021-11-05T12:45:22.299442Z","iopub.status.idle":"2021-11-05T12:45:22.313277Z","shell.execute_reply.started":"2021-11-05T12:45:22.299404Z","shell.execute_reply":"2021-11-05T12:45:22.312423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **df_oil**:","metadata":{}},{"cell_type":"code","source":"df_oil.head(3) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.314962Z","iopub.execute_input":"2021-11-05T12:45:22.315263Z","iopub.status.idle":"2021-11-05T12:45:22.329426Z","shell.execute_reply.started":"2021-11-05T12:45:22.315226Z","shell.execute_reply":"2021-11-05T12:45:22.328545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **df_stores**:","metadata":{}},{"cell_type":"code","source":"df_stores.head(10) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.331301Z","iopub.execute_input":"2021-11-05T12:45:22.331686Z","iopub.status.idle":"2021-11-05T12:45:22.348727Z","shell.execute_reply.started":"2021-11-05T12:45:22.331648Z","shell.execute_reply":"2021-11-05T12:45:22.34795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **df_trans**:","metadata":{}},{"cell_type":"code","source":"df_trans.head(3) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.3501Z","iopub.execute_input":"2021-11-05T12:45:22.350718Z","iopub.status.idle":"2021-11-05T12:45:22.365123Z","shell.execute_reply.started":"2021-11-05T12:45:22.350687Z","shell.execute_reply":"2021-11-05T12:45:22.364245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **df_train**:","metadata":{}},{"cell_type":"code","source":"df_train.head(10) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.367892Z","iopub.execute_input":"2021-11-05T12:45:22.368622Z","iopub.status.idle":"2021-11-05T12:45:22.384848Z","shell.execute_reply.started":"2021-11-05T12:45:22.368583Z","shell.execute_reply":"2021-11-05T12:45:22.3841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **df_test**:","metadata":{}},{"cell_type":"code","source":"df_test.head(5) # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.388117Z","iopub.execute_input":"2021-11-05T12:45:22.388892Z","iopub.status.idle":"2021-11-05T12:45:22.40211Z","shell.execute_reply.started":"2021-11-05T12:45:22.388848Z","shell.execute_reply":"2021-11-05T12:45:22.401094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Visualize data\nOne of the biggest parts of the notebook. Here we can look through some variables and see some dependencies. Firstly, let's check the **dependency of the oil from the date**:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(25,15))\ndf_oil.plot.line(x=\"date\", y=\"dcoilwtico\", color='b', title =\"dcoilwtico\", ax = axes, rot=0)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.404113Z","iopub.execute_input":"2021-11-05T12:45:22.404889Z","iopub.status.idle":"2021-11-05T12:45:22.724228Z","shell.execute_reply.started":"2021-11-05T12:45:22.404845Z","shell.execute_reply":"2021-11-05T12:45:22.723476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by **mean**.","metadata":{}},{"cell_type":"code","source":"def grouped(df, key, freq, col):\n    \"\"\" GROUP DATA WITH CERTAIN FREQUENCY \"\"\"\n    df_grouped = df.groupby([pd.Grouper(key=key, freq=freq)]).agg(mean = (col, 'mean'))\n    df_grouped = df_grouped.reset_index()\n    return df_grouped","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.725619Z","iopub.execute_input":"2021-11-05T12:45:22.726154Z","iopub.status.idle":"2021-11-05T12:45:22.731793Z","shell.execute_reply.started":"2021-11-05T12:45:22.726042Z","shell.execute_reply":"2021-11-05T12:45:22.730802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **check grouped data**:","metadata":{}},{"cell_type":"code","source":"# check grouped data\ndf_grouped_trans_w = grouped(df_trans, 'date', 'W', 'transactions')\ndf_grouped_trans_w","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.733382Z","iopub.execute_input":"2021-11-05T12:45:22.733839Z","iopub.status.idle":"2021-11-05T12:45:22.764779Z","shell.execute_reply.started":"2021-11-05T12:45:22.733801Z","shell.execute_reply":"2021-11-05T12:45:22.763926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And, for better forecasting we'll add ***'time'*** column to our dataframe.","metadata":{}},{"cell_type":"code","source":"def add_time(df, key, freq, col):\n    \"\"\" ADD COLUMN 'TIME' TO DF \"\"\"\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['time'] = np.arange(len(df_grouped.index))\n    column_time = df_grouped.pop('time')\n    df_grouped.insert(1, 'time', column_time)\n    return df_grouped","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.766317Z","iopub.execute_input":"2021-11-05T12:45:22.766621Z","iopub.status.idle":"2021-11-05T12:45:22.773182Z","shell.execute_reply.started":"2021-11-05T12:45:22.766584Z","shell.execute_reply":"2021-11-05T12:45:22.772383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now we can check the results of grouping on the example of **df_train (grouped by weeks on sales, after that, mean was counted)**.","metadata":{}},{"cell_type":"code","source":"df_grouped_train_w = add_time(df_train, 'date', 'W', 'sales')\ndf_grouped_train_m = add_time(df_train, 'date', 'M', 'sales')\n\ndf_grouped_train_w.head() # check results","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.774844Z","iopub.execute_input":"2021-11-05T12:45:22.77541Z","iopub.status.idle":"2021-11-05T12:45:22.902428Z","shell.execute_reply.started":"2021-11-05T12:45:22.775371Z","shell.execute_reply":"2021-11-05T12:45:22.901521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1. Linear Regression\nAfter that, we can build some more plots. **Linear regression** is widely used in practice and adapts naturally to even complex forecasting tasks. The linear regression algorithm learns how to make a weighted sum from its input features.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(30,20))\n\n# TRANSACTIONS (WEEKLY)\naxes[0].plot('date', 'mean', data=df_grouped_trans_w, color='grey', marker='o')\naxes[0].set_title(\"Transactions (grouped by week)\", fontsize=20)\n\n# SALES (WEEKLY)\naxes[1].plot('time', 'mean', data=df_grouped_train_w, color='0.75')\naxes[1].set_title(\"Sales (grouped by week)\", fontsize=20)\n# linear regression\naxes[1] = sns.regplot(x='time', \n                      y='mean', \n                      data=df_grouped_train_w, \n                      scatter_kws=dict(color='0.75'), \n                      ax = axes[1])\n\n# SALES (MONTHLY)\naxes[2].plot('time', 'mean', data=df_grouped_train_m, color='0.75')\naxes[2].set_title(\"Sales (grouped by month)\", fontsize=20)\n# linear regression\naxes[2] = sns.regplot(x='time', \n                      y='mean', \n                      data=df_grouped_train_m, \n                      scatter_kws=dict(color='0.75'), \n                      line_kws={\"color\": \"red\"},\n                      ax = axes[2])\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:22.90407Z","iopub.execute_input":"2021-11-05T12:45:22.904376Z","iopub.status.idle":"2021-11-05T12:45:23.690604Z","shell.execute_reply.started":"2021-11-05T12:45:22.904337Z","shell.execute_reply":"2021-11-05T12:45:23.687324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 Lag feature\nTo make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we've created a 1-step lag feature, though shifting by multiple steps is possible too. So, firstly, we should **add lag** to our data:","metadata":{}},{"cell_type":"code","source":"def add_lag(df, key, freq, col, lag):\n    \"\"\" ADD LAG \"\"\"\n    df_grouped = grouped(df, key, freq, col)\n    name = 'Lag_' + str(lag)\n    df_grouped['Lag'] = df_grouped['mean'].shift(lag)\n    return df_grouped","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:45:23.691891Z","iopub.execute_input":"2021-11-05T12:45:23.692293Z","iopub.status.idle":"2021-11-05T12:45:23.697534Z","shell.execute_reply.started":"2021-11-05T12:45:23.692257Z","shell.execute_reply":"2021-11-05T12:45:23.696707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **check grouped data with lag**:","metadata":{}},{"cell_type":"code","source":"df_grouped_train_w_lag1 = add_lag(df_train, 'date', 'W', 'sales', 1)\ndf_grouped_train_m_lag1 = add_lag(df_train, 'date', 'W', 'sales', 1)\n\ndf_grouped_train_w_lag1.head() # check data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:23.698987Z","iopub.execute_input":"2021-11-05T12:45:23.699673Z","iopub.status.idle":"2021-11-05T12:45:23.838851Z","shell.execute_reply.started":"2021-11-05T12:45:23.699622Z","shell.execute_reply":"2021-11-05T12:45:23.838094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let's build same plots, but with ***'lag'*** feature:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,20))\naxes[0].plot('Lag', 'mean', data=df_grouped_train_w_lag1, color='0.75', linestyle=(0, (1, 10)))\naxes[0].set_title(\"Sales (grouped by week)\", fontsize=20)\naxes[0] = sns.regplot(x='Lag', \n                      y='mean', \n                      data=df_grouped_train_w_lag1, \n                      scatter_kws=dict(color='0.75'), \n                      ax = axes[0])\n\n\naxes[1].plot('Lag', 'mean', data=df_grouped_train_m_lag1, color='0.75', linestyle=(0, (1, 10)))\naxes[1].set_title(\"Sales (grouped by month)\", fontsize=20)\naxes[1] = sns.regplot(x='Lag', \n                      y='mean', \n                      data=df_grouped_train_m_lag1, \n                      scatter_kws=dict(color='0.75'), \n                      line_kws={\"color\": \"red\"},\n                      ax = axes[1])\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:23.840194Z","iopub.execute_input":"2021-11-05T12:45:23.840753Z","iopub.status.idle":"2021-11-05T12:45:24.479541Z","shell.execute_reply.started":"2021-11-05T12:45:23.840712Z","shell.execute_reply":"2021-11-05T12:45:24.478738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3 Some more statistics & visualizations\nIn this block we are going to explore data. Firstly, let's count for each category in each dataset, ***value_counts()***:","metadata":{}},{"cell_type":"code","source":"def plot_stats(df, column, ax, color, angle):\n    \"\"\" PLOT STATS OF DIFFERENT COLUMNS \"\"\"\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=18)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:24.480826Z","iopub.execute_input":"2021-11-05T12:45:24.481651Z","iopub.status.idle":"2021-11-05T12:45:24.487448Z","shell.execute_reply.started":"2021-11-05T12:45:24.481612Z","shell.execute_reply":"2021-11-05T12:45:24.486611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see stats for **df_holidays**:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\nfig.autofmt_xdate()\nfig.suptitle(\"Stats of df_holidays\".upper())\nplot_stats(df_holidays, \"type\", axes[0], \"pastel\", 45)\nplot_stats(df_holidays, \"locale\", axes[1], \"rocket\", 45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:24.488675Z","iopub.execute_input":"2021-11-05T12:45:24.489398Z","iopub.status.idle":"2021-11-05T12:45:24.803778Z","shell.execute_reply.started":"2021-11-05T12:45:24.489363Z","shell.execute_reply":"2021-11-05T12:45:24.803059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we count values for some columns of **df_stores**:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,40))\nplot_stats(df_stores, \"city\", axes[0], \"mako_r\", 45)\nplot_stats(df_stores, \"state\", axes[1], \"rocket_r\", 45)\nplot_stats(df_stores, \"type\", axes[2], \"magma\", 0)\nplot_stats(df_stores, \"cluster\", axes[3], \"viridis\", 0)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:24.80518Z","iopub.execute_input":"2021-11-05T12:45:24.80564Z","iopub.status.idle":"2021-11-05T12:45:25.959358Z","shell.execute_reply.started":"2021-11-05T12:45:24.805604Z","shell.execute_reply":"2021-11-05T12:45:25.958601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot **pie chart** for ***'family'*** of **df_train**:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ncount_classes = df_train['family'].value_counts()\nplt.title(\"Stats of df_train\".upper())\ncolors = ['#ff9999','#66b3ff','#99ff99',\n          '#ffcc99', '#ffccf9', '#ff99f8', \n          '#ff99af', '#ffe299', '#a8ff99',\n          '#cc99ff', '#9e99ff', '#99c9ff',\n          '#99f5ff', '#99ffe4', '#99ffaf']\n\nplt.pie(count_classes, \n        labels = count_classes.index, \n        autopct='%1.1f%%',\n        shadow=True, \n        startangle=90, \n        colors=colors)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:25.960715Z","iopub.execute_input":"2021-11-05T12:45:25.961106Z","iopub.status.idle":"2021-11-05T12:45:27.118707Z","shell.execute_reply.started":"2021-11-05T12:45:25.961071Z","shell.execute_reply":"2021-11-05T12:45:27.114565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.4 BoxPlot\nIn addition, we can build some **boxplots**: for **df_oil** & **df_trans**.","metadata":{}},{"cell_type":"code","source":"def plot_boxplot(palette, x, y, hue, ax, title):\n    sns.set_theme(style=\"ticks\", palette=palette)\n    ax = sns.boxplot(x=x, y=y, hue=hue, ax=ax)\n    ax.set_title(title, fontsize=18)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:27.120104Z","iopub.execute_input":"2021-11-05T12:45:27.120897Z","iopub.status.idle":"2021-11-05T12:45:27.126553Z","shell.execute_reply.started":"2021-11-05T12:45:27.120857Z","shell.execute_reply":"2021-11-05T12:45:27.125803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30,60))\nplot_boxplot(\"pastel\", df_oil['date'].dt.year, df_oil['dcoilwtico'], df_oil['date'].dt.month, axes[0], \"df_oil\")\nplot_boxplot(\"pastel\", df_oil['date'].dt.year, df_oil['dcoilwtico'], df_oil['date'].dt.year, axes[1], \"df_oil\")\nplot_boxplot(\"pastel\", df_trans['date'].dt.year, df_trans['transactions'], df_trans['date'].dt.month, axes[2], \"df_trans\")\nplot_boxplot(\"pastel\", df_trans['date'].dt.year, df_trans['transactions'], df_trans['date'].dt.year, axes[3], \"df_trans\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:27.12794Z","iopub.execute_input":"2021-11-05T12:45:27.128682Z","iopub.status.idle":"2021-11-05T12:45:30.004606Z","shell.execute_reply.started":"2021-11-05T12:45:27.128646Z","shell.execute_reply":"2021-11-05T12:45:30.003766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.5 Trend. Moving Average\nThe **trend** component of a time series represents a persistent, long-term change in the mean of the series. The trend is the slowest-moving part of a series, the part representing the largest time scale of importance. In a time series of product sales, an increasing trend might be the effect of a market expansion as more people become aware of the product year by year.\n\nTo see what kind of trend a time series might have, we can use a **moving average** plot. To compute a moving average of a time series, we compute the average of the values within a sliding window of some defined width. Each point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to smooth out any short-term fluctuations in the series so that only long-term changes remain.\n\nBelow we can see the moving average plots for **Transactions** and **Sales**, colored in green.","metadata":{}},{"cell_type":"code","source":"def plot_moving_average(df, key, freq, col, window, min_periods, ax, title):\n    df_grouped = grouped(df, key, freq, col)\n    moving_average = df_grouped['mean'].rolling(window=window, center=True, min_periods=min_periods).mean()   \n    ax = df_grouped['mean'].plot(color='0.75', linestyle='dashdot', ax=ax)\n    ax = moving_average.plot(linewidth=3, color='g', ax=ax)\n    ax.set_title(title, fontsize=18)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:30.005965Z","iopub.execute_input":"2021-11-05T12:45:30.006572Z","iopub.status.idle":"2021-11-05T12:45:30.014109Z","shell.execute_reply.started":"2021-11-05T12:45:30.006534Z","shell.execute_reply":"2021-11-05T12:45:30.013264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,20))\nplot_moving_average(df_trans, 'date', 'W', 'transactions', 7, 4, axes[0], 'Transactions Moving Average')\nplot_moving_average(df_train, 'date', 'W', 'sales', 7, 4, axes[1], 'Sales Moving Average')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:30.015751Z","iopub.execute_input":"2021-11-05T12:45:30.016014Z","iopub.status.idle":"2021-11-05T12:45:30.540178Z","shell.execute_reply.started":"2021-11-05T12:45:30.01598Z","shell.execute_reply":"2021-11-05T12:45:30.539526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.6. Trend. Forecasting Trend\nWe'll use a function from the **statsmodels** library called **DeterministicProcess**. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.","metadata":{}},{"cell_type":"code","source":"def plot_deterministic_process(df, key, freq, col, ax1, title1, ax2, title2):\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\") \n    dp = DeterministicProcess(index=df_grouped['date'], constant=True, order=1, drop=True)\n    dp.index.freq = freq # manually set the frequency of the index\n    # 'in_sample' creates features for the dates given in the `index` argument\n    X1 = dp.in_sample()\n    y1 = df_grouped[\"mean\"]  # the target\n    y1.index = X1.index\n    # The intercept is the same as the `const` feature from\n    # DeterministicProcess. LinearRegression behaves badly with duplicated\n    # features, so we need to be sure to exclude it here.\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X1, y1)\n    y1_pred = pd.Series(model.predict(X1), index=X1.index)\n    ax1 = y1.plot(linestyle='dashed', label=\"mean\", color=\"0.75\", ax=ax1, use_index=True)\n    ax1 = y1_pred.plot(linewidth=3, label=\"Trend\", color='b', ax=ax1, use_index=True)\n    ax1.set_title(title1, fontsize=18)  \n    _ = ax1.legend()\n    \n    # forecast Trend for future 30 steps\n    steps = 30 \n    X2 = dp.out_of_sample(steps=steps)\n    y2_fore = pd.Series(model.predict(X2), index=X2.index)\n    y2_fore.head()\n    ax2 = y1.plot(linestyle='dashed', label=\"mean\", color=\"0.75\", ax=ax2, use_index=True)\n    ax2 = y1_pred.plot(linewidth=3, label=\"Trend\", color='b', ax=ax2, use_index=True)\n    ax2 = y2_fore.plot(linewidth=3, label=\"Predicted Trend\", color='r', ax=ax2, use_index=True)\n    ax2.set_title(title2, fontsize=18)  \n    _ = ax2.legend()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:30.54552Z","iopub.execute_input":"2021-11-05T12:45:30.546345Z","iopub.status.idle":"2021-11-05T12:45:30.561216Z","shell.execute_reply.started":"2021-11-05T12:45:30.546306Z","shell.execute_reply":"2021-11-05T12:45:30.560544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see **Linear Trend** & **Linear Trend Forecast** for **Transactions** (plots 1,2) and **Sales** (plots 3,4).","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30,30))\nplot_deterministic_process(df_trans, 'date', 'W', 'transactions', \n                           axes[0], \"Transactions Linear Trend\",  \n                           axes[1], \"Transactions Linear Trend Forecast\")\nplot_deterministic_process(df_train, 'date', 'W', 'sales', \n                           axes[2], \"Sales Linear Trend\", \n                           axes[3], \"Sales Linear Trend Forecast\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:30.562833Z","iopub.execute_input":"2021-11-05T12:45:30.563037Z","iopub.status.idle":"2021-11-05T12:45:31.642641Z","shell.execute_reply.started":"2021-11-05T12:45:30.56301Z","shell.execute_reply":"2021-11-05T12:45:31.642048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.7 Seasonality\nTime series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times. Just like we used a moving average plot to discover the trend in a series, we can use a **seasonal plot** to discover seasonal patterns.","metadata":{}},{"cell_type":"code","source":"def seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(x=X[freq], \n                      y=X[y],\n                      ax=ax, \n                      hue=X[period],\n                      palette=palette, \n                      legend=False)\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(name, \n                    xy=(1, y_), \n                    xytext=(6, 0), \n                    color=line.get_color(), \n                    xycoords=ax.get_yaxis_transform(), \n                    textcoords=\"offset points\", \n                    size=14, \n                    va=\"center\")\n    return ax","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:31.643859Z","iopub.execute_input":"2021-11-05T12:45:31.644415Z","iopub.status.idle":"2021-11-05T12:45:31.65436Z","shell.execute_reply.started":"2021-11-05T12:45:31.644377Z","shell.execute_reply":"2021-11-05T12:45:31.65344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(ts, fs=fs, detrend=detrend, window=\"boxcar\", scaling='spectrum')\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels([\"Annual (1)\", \"Semiannual (2)\", \"Quarterly (4)\", \n                        \"Bimonthly (6)\", \"Monthly (12)\", \"Biweekly (26)\", \n                        \"Weekly (52)\", \"Semiweekly (104)\"], rotation=30)\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:31.655673Z","iopub.execute_input":"2021-11-05T12:45:31.65658Z","iopub.status.idle":"2021-11-05T12:45:31.666895Z","shell.execute_reply.started":"2021-11-05T12:45:31.656543Z","shell.execute_reply":"2021-11-05T12:45:31.666143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seasonality(df, key, freq, col):\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\")\n    df_grouped.index = df_grouped['date'] \n    df_grouped = df_grouped.drop(columns=['date'])\n    df_grouped.index.freq = freq # manually set the frequency of the index\n    \n    X = df_grouped.copy()\n    X.index = pd.to_datetime(X.index, format = \"%Y-%m-%d\") \n    X.index.freq = freq \n    # days within a week\n    X[\"day\"] = X.index.dayofweek   # the x-axis (freq)\n    X[\"week\"] = pd.Int64Index(X.index.isocalendar().week)  # the seasonal period (period)\n    # days within a year\n    X[\"dayofyear\"] = X.index.dayofyear\n    X[\"year\"] = X.index.year\n    fig, (ax0, ax1, ax2) = plt.subplots(3, 1, figsize=(20, 30))\n    seasonal_plot(X, y='mean', period=\"week\", freq=\"day\", ax=ax0)\n    seasonal_plot(X, y='mean', period=\"year\", freq=\"dayofyear\", ax=ax1)\n    X_new = (X['mean'].copy()).dropna()\n    plot_periodogram(X_new, ax=ax2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:31.66847Z","iopub.execute_input":"2021-11-05T12:45:31.668876Z","iopub.status.idle":"2021-11-05T12:45:31.682376Z","shell.execute_reply.started":"2021-11-05T12:45:31.668837Z","shell.execute_reply":"2021-11-05T12:45:31.681446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at **seasonal plots** over a week and over a year. Here we can see the plots for **df_trans**.","metadata":{}},{"cell_type":"code","source":"# df_trans, grouped by day\nseasonality(df_trans, 'date', 'D', 'transactions')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:31.68374Z","iopub.execute_input":"2021-11-05T12:45:31.684582Z","iopub.status.idle":"2021-11-05T12:45:43.21783Z","shell.execute_reply.started":"2021-11-05T12:45:31.684548Z","shell.execute_reply":"2021-11-05T12:45:43.217067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the plots for **df_train**.","metadata":{}},{"cell_type":"code","source":"# df_train, grouped by day\nseasonality(df_train, 'date', 'D', 'sales')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:43.219138Z","iopub.execute_input":"2021-11-05T12:45:43.219801Z","iopub.status.idle":"2021-11-05T12:45:55.511587Z","shell.execute_reply.started":"2021-11-05T12:45:43.219757Z","shell.execute_reply":"2021-11-05T12:45:55.510031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can **predict seasonality**, using **DeterministicProcess**, as we used for Trend. We are going to forecast seasonality for **Transactions** and **Sales**.","metadata":{}},{"cell_type":"code","source":"def predict_seasonality(df, key, freq, col, ax1, title1):\n    fourier = CalendarFourier(freq=\"A\", order=10)  # 10 sin/cos pairs for \"A\"nnual seasonality\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\") \n    df_grouped['date'].freq = freq # manually set the frequency of the index\n    dp = DeterministicProcess(index=df_grouped['date'], \n                              constant=True, \n                              order=1, \n                              period=None, \n                              seasonal=True, \n                              additional_terms=[fourier], \n                              drop=True)\n    dp.index.freq = freq # manually set the frequency of the index\n\n    # 'in_sample' creates features for the dates given in the `index` argument\n    X1 = dp.in_sample()\n    y1 = df_grouped[\"mean\"]  # the target\n    y1.index = X1.index\n\n    # The intercept is the same as the `const` feature from\n    # DeterministicProcess. LinearRegression behaves badly with duplicated\n    # features, so we need to be sure to exclude it here.\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X1, y1)\n    y1_pred = pd.Series(model.predict(X1), index=X1.index)\n    X1_fore = dp.out_of_sample(steps=90)\n    y1_fore = pd.Series(model.predict(X1_fore), index=X1_fore.index)\n    \n    ax1 = y1.plot(linestyle='dashed', style='.', label=\"init mean values\", color=\"0.4\", ax=ax1, use_index=True)\n    ax1 = y1_pred.plot(linewidth=3, label=\"Seasonal\", color='b', ax=ax1, use_index=True)\n    ax1 = y1_fore.plot(linewidth=3, label=\"Seasonal Forecast\", color='r', ax=ax1, use_index=True)\n    ax1.set_title(title1, fontsize=18)  \n    _ = ax1.legend()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:55.512814Z","iopub.execute_input":"2021-11-05T12:45:55.513351Z","iopub.status.idle":"2021-11-05T12:45:55.526649Z","shell.execute_reply.started":"2021-11-05T12:45:55.513311Z","shell.execute_reply":"2021-11-05T12:45:55.525899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,30))\npredict_seasonality(df_trans, 'date', 'W', 'transactions', axes[0], \"Transactions Seasonal Forecast\")\npredict_seasonality(df_train, 'date', 'W', 'sales', axes[1], \"Sales Seasonal Forecast\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:55.527831Z","iopub.execute_input":"2021-11-05T12:45:55.528684Z","iopub.status.idle":"2021-11-05T12:45:56.764759Z","shell.execute_reply.started":"2021-11-05T12:45:55.528637Z","shell.execute_reply":"2021-11-05T12:45:56.764045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Time Series as Features","metadata":{}},{"cell_type":"code","source":"store_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean() \n    .unstack('family')\n    .loc['2017', ['sales', 'onpromotion']]\n)\n\nmag_sales = family_sales.loc(axis=1)[:, 'MAGAZINES']","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:45:56.765957Z","iopub.execute_input":"2021-11-05T12:45:56.768347Z","iopub.status.idle":"2021-11-05T12:45:58.792538Z","shell.execute_reply.started":"2021-11-05T12:45:56.768308Z","shell.execute_reply":"2021-11-05T12:45:58.791688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **check data store_sales**:","metadata":{}},{"cell_type":"code","source":"store_sales.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:58.794097Z","iopub.execute_input":"2021-11-05T12:45:58.79439Z","iopub.status.idle":"2021-11-05T12:45:58.809328Z","shell.execute_reply.started":"2021-11-05T12:45:58.794353Z","shell.execute_reply":"2021-11-05T12:45:58.808567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **check data mag_sales**:","metadata":{}},{"cell_type":"code","source":"mag_sales.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:58.810725Z","iopub.execute_input":"2021-11-05T12:45:58.811155Z","iopub.status.idle":"2021-11-05T12:45:58.823489Z","shell.execute_reply.started":"2021-11-05T12:45:58.811118Z","shell.execute_reply":"2021-11-05T12:45:58.822699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **plot data**:","metadata":{}},{"cell_type":"code","source":"y = mag_sales.loc[:, 'sales'].squeeze()\n\nfourier = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(\n    constant=True,\n    index=y.index,\n    order=1,\n    seasonal=True,\n    drop=True,\n    additional_terms=[fourier],\n)\nX_time = dp.in_sample()\nX_time['NewYearsDay'] = (X_time.index.dayofyear == 1)\n\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_time, y)\ny_deseason = y - model.predict(X_time)\ny_deseason.name = 'sales_deseasoned'\n\nax = y_deseason.plot()\nax.set_title(\"Magazine Sales (deseasonalized)\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:58.824792Z","iopub.execute_input":"2021-11-05T12:45:58.825293Z","iopub.status.idle":"2021-11-05T12:45:59.177744Z","shell.execute_reply.started":"2021-11-05T12:45:58.825256Z","shell.execute_reply":"2021-11-05T12:45:59.177078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict (in the same row, in other words). This makes lagged series useful as features for modeling serial dependence. To forecast series, we could use y_lag_1 and y_lag_2 as features to predict the target y.","metadata":{}},{"cell_type":"markdown","source":"# 4.1 Lag plot\nA lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. The most commonly used measure of serial dependence is known as **autocorrelation**, which is simply the correlation a time series has with one of its lags. The **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of \"new\" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use.\n","metadata":{}},{"cell_type":"code","source":"def lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(alpha=0.75,s=3)\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_, y=y_, scatter_kws=scatter_kws, line_kws=line_kws, lowess=True, ax=ax, **kwargs)\n    at = AnchoredText(f\"{corr:.2f}\",prop=dict(size=\"large\"), frameon=True, loc=\"upper left\")\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2 + 10, nrows * 2 + 5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:59.178977Z","iopub.execute_input":"2021-11-05T12:45:59.179746Z","iopub.status.idle":"2021-11-05T12:45:59.195106Z","shell.execute_reply.started":"2021-11-05T12:45:59.179709Z","shell.execute_reply":"2021-11-05T12:45:59.194458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the **lag** and **autocorrelation plots** first:","metadata":{}},{"cell_type":"code","source":"_ = plot_lags(y_deseason, lags=8, nrows=2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:45:59.196575Z","iopub.execute_input":"2021-11-05T12:45:59.196916Z","iopub.status.idle":"2021-11-05T12:46:00.750853Z","shell.execute_reply.started":"2021-11-05T12:45:59.196879Z","shell.execute_reply":"2021-11-05T12:46:00.750054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = plot_pacf(y_deseason, lags=8)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:00.752583Z","iopub.execute_input":"2021-11-05T12:46:00.753103Z","iopub.status.idle":"2021-11-05T12:46:00.969433Z","shell.execute_reply.started":"2021-11-05T12:46:00.753065Z","shell.execute_reply":"2021-11-05T12:46:00.968725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we examine leading and lagging values for **onpromotion** plotted against **magazine sales**.","metadata":{}},{"cell_type":"code","source":"onpromotion = mag_sales.loc[:, 'onpromotion'].squeeze().rename('onpromotion')\n\n# Drop the New Year outlier\nplot_lags(x=onpromotion.iloc[1:], y=y_deseason.iloc[1:], lags=3, nrows=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:00.970681Z","iopub.execute_input":"2021-11-05T12:46:00.971094Z","iopub.status.idle":"2021-11-05T12:46:01.841092Z","shell.execute_reply.started":"2021-11-05T12:46:00.971055Z","shell.execute_reply":"2021-11-05T12:46:01.840187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Lags. Forecasting\nAfter that, we can **make lags** for future plots.","metadata":{}},{"cell_type":"code","source":"def make_lags(ts, lags):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(1, lags + 1)\n        },\n        axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:01.842709Z","iopub.execute_input":"2021-11-05T12:46:01.84297Z","iopub.status.idle":"2021-11-05T12:46:01.848753Z","shell.execute_reply.started":"2021-11-05T12:46:01.842936Z","shell.execute_reply":"2021-11-05T12:46:01.847684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = make_lags(y_deseason, lags=4)\nX = X.fillna(0.0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:01.850441Z","iopub.execute_input":"2021-11-05T12:46:01.850729Z","iopub.status.idle":"2021-11-05T12:46:01.864965Z","shell.execute_reply.started":"2021-11-05T12:46:01.850693Z","shell.execute_reply":"2021-11-05T12:46:01.864257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can build plot with **predictions**:","metadata":{}},{"cell_type":"code","source":"# Create target series and data splits\ny = y_deseason.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False)\n\n# Fit and predict\nmodel = LinearRegression()  # `fit_intercept=True` since we didn't use DeterministicProcess\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nax = y_train.plot(color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", ax=ax)\nax = y_test.plot(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", ax=ax)\nax = y_pred.plot(ax=ax)\n_ = y_fore.plot(ax=ax, color='C3')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:01.86872Z","iopub.execute_input":"2021-11-05T12:46:01.86937Z","iopub.status.idle":"2021-11-05T12:46:02.240717Z","shell.execute_reply.started":"2021-11-05T12:46:01.869328Z","shell.execute_reply":"2021-11-05T12:46:02.240031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. Here we'll learn how to create **\"hybrid\" forecasters** that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other.","metadata":{}},{"cell_type":"code","source":"store_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:02.24187Z","iopub.execute_input":"2021-11-05T12:46:02.242604Z","iopub.status.idle":"2021-11-05T12:46:04.22955Z","shell.execute_reply.started":"2021-11-05T12:46:02.242569Z","shell.execute_reply":"2021-11-05T12:46:04.228721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Firslty, we should create **Boosted Hybrid class**:","metadata":{}},{"cell_type":"code","source":"# we'll add fit and predict methods to this minimal class\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:04.231087Z","iopub.execute_input":"2021-11-05T12:46:04.231524Z","iopub.status.idle":"2021-11-05T12:46:04.237434Z","shell.execute_reply.started":"2021-11-05T12:46:04.23148Z","shell.execute_reply":"2021-11-05T12:46:04.236587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we need to create **fit** method:","metadata":{}},{"cell_type":"code","source":"def fit(self, X_1, X_2, y):\n    # train model_1\n    self.model_1.fit(X_1, y)\n\n    # make predictions\n    y_fit = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=y.columns,\n    )\n\n    # compute residuals\n    y_resid = y - y_fit\n    y_resid = y_resid.stack().squeeze() # wide to long\n\n    # train model_2 on residuals\n    self.model_2.fit(X_2, y_resid)\n\n    # save column names for predict method\n    self.y_columns = y.columns\n    # Save data for question checking\n    self.y_fit = y_fit\n    self.y_resid = y_resid\n\n\n# Add method to class\nBoostedHybrid.fit = fit","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:04.239349Z","iopub.execute_input":"2021-11-05T12:46:04.239758Z","iopub.status.idle":"2021-11-05T12:46:04.249057Z","shell.execute_reply.started":"2021-11-05T12:46:04.239721Z","shell.execute_reply":"2021-11-05T12:46:04.24822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And **predict** method:","metadata":{}},{"cell_type":"code","source":"def predict(self, X_1, X_2):\n    # Predict with model_1\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=self.y_columns,\n    )\n    y_pred = y_pred.stack().squeeze()  # wide to long\n\n    # Add model_2 predictions to model_1 predictions\n    y_pred += self.model_2.predict(X_2)\n\n    return y_pred.unstack()\n\n\n# Add method to class\nBoostedHybrid.predict = predict","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:04.250736Z","iopub.execute_input":"2021-11-05T12:46:04.251102Z","iopub.status.idle":"2021-11-05T12:46:04.262023Z","shell.execute_reply.started":"2021-11-05T12:46:04.25099Z","shell.execute_reply":"2021-11-05T12:46:04.26123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **set up data for training**:","metadata":{}},{"cell_type":"code","source":"# Target series\ny = family_sales.loc[:, 'sales']\n\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\n\n# X_2: Features for XGBoost\nX_2 = family_sales.drop('sales', axis=1).stack()  # onpromotion feature\n\n# Label encoding for 'family'\nle = LabelEncoder()  # from sklearn.preprocessing\nX_2 = X_2.reset_index('family')\nX_2['family'] = le.fit_transform(X_2['family'])\n\n# Label encoding for seasonality\nX_2[\"day\"] = X_2.index.day  # values are day of the month","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:04.263731Z","iopub.execute_input":"2021-11-05T12:46:04.264475Z","iopub.status.idle":"2021-11-05T12:46:04.488284Z","shell.execute_reply.started":"2021-11-05T12:46:04.264435Z","shell.execute_reply":"2021-11-05T12:46:04.487583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can **train** our model:","metadata":{}},{"cell_type":"code","source":"# Create model\nmodel = BoostedHybrid(\n    model_1=LinearRegression(),\n    model_2=XGBRegressor())\n\nmodel.fit(X_1, X_2, y)\n\ny_pred = model.predict(X_1, X_2)\ny_pred = y_pred.clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:04.490286Z","iopub.execute_input":"2021-11-05T12:46:04.490847Z","iopub.status.idle":"2021-11-05T12:46:05.605858Z","shell.execute_reply.started":"2021-11-05T12:46:04.490809Z","shell.execute_reply":"2021-11-05T12:46:05.60511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boosted Hybrid\nmodel = BoostedHybrid(\n    model_1=Ridge(),\n    model_2=KNeighborsRegressor(),\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:05.607418Z","iopub.execute_input":"2021-11-05T12:46:05.607984Z","iopub.status.idle":"2021-11-05T12:46:05.612011Z","shell.execute_reply.started":"2021-11-05T12:46:05.607945Z","shell.execute_reply":"2021-11-05T12:46:05.611242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that we train and plot","metadata":{}},{"cell_type":"code","source":"y_train, y_valid = y[:\"2017-07-01\"], y[\"2017-07-02\":]\nX1_train, X1_valid = X_1[: \"2017-07-01\"], X_1[\"2017-07-02\" :]\nX2_train, X2_valid = X_2.loc[:\"2017-07-01\"], X_2.loc[\"2017-07-02\":]\n\n# Some of the algorithms above do best with certain kinds of\n# preprocessing on the features (like standardization), but this is\n# just a demo.\nmodel.fit(X1_train, X2_train, y_train)\ny_fit = model.predict(X1_train, X2_train).clip(0.0)\ny_pred = model.predict(X1_valid, X2_valid).clip(0.0)\n\nfamilies = y.columns[0:6]\naxs = y.loc(axis=1)[families].plot(subplots=True, \n                                   sharex=True, \n                                   figsize=(30, 20), \n                                   color=\"0.75\",\n                                   style=\".-\",\n                                   markeredgecolor=\"0.25\",\n                                   markerfacecolor=\"0.25\",\n                                   alpha=0.5)\n_ = y_fit.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C0', ax=axs)\n_ = y_pred.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C3', ax=axs)\nfor ax, family in zip(axs, families):\n    ax.legend([])\n    ax.set_ylabel(family)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:05.613634Z","iopub.execute_input":"2021-11-05T12:46:05.614173Z","iopub.status.idle":"2021-11-05T12:46:07.476603Z","shell.execute_reply.started":"2021-11-05T12:46:05.614137Z","shell.execute_reply":"2021-11-05T12:46:07.475024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Machine learning forecasting","metadata":{}},{"cell_type":"code","source":"# train data\nstore_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:07.47808Z","iopub.execute_input":"2021-11-05T12:46:07.478605Z","iopub.status.idle":"2021-11-05T12:46:09.491998Z","shell.execute_reply.started":"2021-11-05T12:46:07.478567Z","shell.execute_reply":"2021-11-05T12:46:09.491243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data\ntest = df_test.copy()\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:09.493174Z","iopub.execute_input":"2021-11-05T12:46:09.493466Z","iopub.status.idle":"2021-11-05T12:46:09.515211Z","shell.execute_reply.started":"2021-11-05T12:46:09.493431Z","shell.execute_reply":"2021-11-05T12:46:09.514584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:09.516421Z","iopub.execute_input":"2021-11-05T12:46:09.516667Z","iopub.status.idle":"2021-11-05T12:46:09.521441Z","shell.execute_reply.started":"2021-11-05T12:46:09.516634Z","shell.execute_reply":"2021-11-05T12:46:09.520577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = family_sales.loc[:, 'sales']\n\n# make 4 lag features\nX = make_lags(y, lags=5).dropna()\n\n# make multistep target\ny = make_multistep_target(y, steps=16).dropna()\n\ny, X = y.align(X, join='inner', axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:09.522963Z","iopub.execute_input":"2021-11-05T12:46:09.523278Z","iopub.status.idle":"2021-11-05T12:46:09.688386Z","shell.execute_reply.started":"2021-11-05T12:46:09.52324Z","shell.execute_reply":"2021-11-05T12:46:09.68766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we **prepare the data** for XGBoost:","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\nX = (X\n    .stack('family')  # wide to long\n    .reset_index('family')  # convert index to column\n    .assign(family=lambda x: le.fit_transform(x.family))  # label encode\n)\ny = y.stack('family')  # wide to long\n\ndisplay(y) ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:09.689458Z","iopub.execute_input":"2021-11-05T12:46:09.689709Z","iopub.status.idle":"2021-11-05T12:46:10.106015Z","shell.execute_reply.started":"2021-11-05T12:46:09.689665Z","shell.execute_reply":"2021-11-05T12:46:10.105117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.1 Forecast with the DirRec strategy\nInstatiate a model that applies the DirRec strategy to XGBoost.","metadata":{}},{"cell_type":"code","source":"# init model\nmodel = RegressorChain(base_estimator=XGBRegressor())","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:10.107684Z","iopub.execute_input":"2021-11-05T12:46:10.107963Z","iopub.status.idle":"2021-11-05T12:46:10.112389Z","shell.execute_reply.started":"2021-11-05T12:46:10.107925Z","shell.execute_reply":"2021-11-05T12:46:10.111486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nmodel.fit(X, y)\ny_pred = pd.DataFrame(model.predict(X), index=y.index,columns=y.columns).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:10.114071Z","iopub.execute_input":"2021-11-05T12:46:10.1144Z","iopub.status.idle":"2021-11-05T12:46:19.360164Z","shell.execute_reply.started":"2021-11-05T12:46:10.11436Z","shell.execute_reply":"2021-11-05T12:46:19.359039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we need to define helpfull function, **plot_multistep**:","metadata":{}},{"cell_type":"code","source":"# helpful function\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax","metadata":{"execution":{"iopub.status.busy":"2021-11-05T12:46:19.361308Z","iopub.status.idle":"2021-11-05T12:46:19.362247Z","shell.execute_reply.started":"2021-11-05T12:46:19.361966Z","shell.execute_reply":"2021-11-05T12:46:19.361992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now, we can **plot results**:","metadata":{}},{"cell_type":"code","source":"FAMILY = 'BEAUTY'\nSTART = '2017-04-01'\nEVERY = 16\n\ny_pred_ = y_pred.xs(FAMILY, level='family', axis=0).loc[START:]\ny_ = family_sales.loc[START:, 'sales'].loc[:, FAMILY]\n\nfig, ax = plt.subplots(1, 1, figsize=(11, 4))\nax = y_.plot(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\", markerfacecolor=\"0.25\",ax=ax, alpha=0.5)\nax = plot_multistep(y_pred_, ax=ax, every=EVERY)\n_ = ax.legend([FAMILY, FAMILY + ' Forecast'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-05T12:46:19.363437Z","iopub.status.idle":"2021-11-05T12:46:19.364287Z","shell.execute_reply.started":"2021-11-05T12:46:19.36402Z","shell.execute_reply":"2021-11-05T12:46:19.364044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion\nThank you for reading my new article!\n\nHope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https://www.kaggle.com/maricinnamon/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https://www.kaggle.com/maricinnamon/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)","metadata":{}},{"cell_type":"markdown","source":"**IF YOU LIKED THIS ARTICLE ABOUT TIME SERIES, PLEASE, MAKE AN UPVOTE ‚ù§Ô∏è**","metadata":{}}]}
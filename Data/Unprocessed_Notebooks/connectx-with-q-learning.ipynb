{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About Q-Learning algorithm\n> \"Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\"\n[*wiki*](https://en.wikipedia.org/wiki/Q-learning)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"ToC\"></a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"install_libraries\"></a>\n# Install libraries\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"import_libraries\"></a>\n# Import libraries\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_useful_classes\"></a>\n# Define useful classes\nNOTE: It's not easy to generate a Q-Table with all possible states; and even if I can do so, the huge number of states will cost much of memory. So, I use the approach that dynamically adding newly discovered states into an object of QTable class created below.\n\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=True)\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n\nclass QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n        \n    def __call__(self, state):\n        board = state.board[:] # Get a copy\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n        \n        return self.table[state_key]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.1\ngamma = 0.6\nepsilon = 0.99\nmin_epsilon = 0.1\n\nepisodes = 10000\n\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"train_the_agent\"></a>\n# Train the agent\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q_table = QTable(env.action_space)\n\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = [] # Last 100 steps\nall_qtable_rows = []\nall_epsilons = []\n\nfor i in tqdm(range(episodes)):\n    state = env.reset()\n\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = choice([c for c in range(env.action_space.n) if state.board[c] == 0])\n        else:\n            row = q_table(state)[:]\n            selected_items = []\n            for j in range(env.action_space.n):\n                if state.board[j] == 0:\n                    selected_items.append(row[j])\n                else:\n                    selected_items.append(-1e7)\n            action = int(np.argmax(selected_items))\n\n        next_state, reward, done, info = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05 # Try to prevent the agent from taking a long move\n\n        old_value = q_table(state)[action]\n        next_max = np.max(q_table(next_state))\n        \n        # Update Q-value\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table(state)[action] = new_value\n\n        state = next_state\n        epochs += 1\n        total_rewards += reward\n\n\n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i-100):(i+1)])\n    all_avg_rewards.append(avg_rewards)\n    all_qtable_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n\n    if (i+1) % alpha_decay_step == 0:\n        alpha *= alpha_decay_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(q_table.table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# for k in q_table.table.keys():\n#     print('State:', k)\n#     print('Action-Value:', list(q_table.table[k]), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plt.plot(all_epochs)\n# plt.xlabel('Episode')\n# plt.ylabel('Num of steps')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_qtable_rows)\nplt.xlabel('Episode')\nplt.ylabel('Explored states')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_an_agent\"></a>\n# Create an Agent\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_agent = '''def my_agent(observation, configuration):\n    from random import choice\n\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    return action\n    '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission import my_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
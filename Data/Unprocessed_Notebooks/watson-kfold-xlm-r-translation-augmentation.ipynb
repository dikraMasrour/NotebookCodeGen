{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"IjZ816ZJK-OQ","outputId":"87ef77c3-0206-41a6-a8aa-b88b6ea3a4c5"},"cell_type":"code","source":"#python basics\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport math, os, re, time, random, json, gc\nimport numpy as np, pandas as pd, seaborn as sns\n\n#deep learning basics\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\n\n#nlp augmentation\n!pip install --quiet googletrans\nfrom googletrans import Translator\n\n#easy way to shuffle rows\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#get current TensorFlow version fo\nprint(\"Currently using Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZTrZNeFEK-OP"},"cell_type":"markdown","source":"# It's Elementary, My Dear Watson\n\n**Natural Language Inference (NLI) is a specific type of NLP task where we must determine whether or not a hypothesis is true based on a premise. Specifically, given a pair of sentences, can we classify them into three different classes: 0 - entailment, 1 - contradiction, 2 - neutral?**\n\n**The current leading model in this field is RoBERTa, described by its creators as a 'robustly optimized BERT pretraining approach'. It changes some of the key hyperparameters of BERT and removes the next-sentence pretraining objective all together. The original paper can be found [here](https://arxiv.org/abs/1907.11692) and the source code [here](https://github.com/pytorch/fairseq/tree/master/examples/roberta)**\n\n**Now, we have 15 different languages in our dataset, so we cannot use the standard pre-trained RoBERTa model as it has only been trained on English sequences. Luckily, there is [XLM-RoBERTa](https://huggingface.co/transformers/model_doc/xlmroberta.html) (original paper can be found [here](https://arxiv.org/abs/1911.02116)) which has been trained on 2.5TB of filtered CommonCrawl data in 100 different languages. The implementation procedude is the same as RoBERTa's, so it is easy enough to deploy. Let's see how:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 34\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'TPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\n#choose batch size - will depend on cores of our device\nBATCH_SIZE = 16 * REPLICAS","execution_count":null,"outputs":[]},{"metadata":{"id":"bUdfpF_2K-OV"},"cell_type":"markdown","source":"# EDA\n\n**A very brief data exploration**"},{"metadata":{"trusted":true,"id":"oE_xjcJNK-OW","outputId":"d33bc71f-7e0d-4786-a7ab-49a8750e1159"},"cell_type":"code","source":"#get CSV files\ntrain = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\nprint(f'Train shape: {train.shape}')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"TXaVG-qQK-OY","outputId":"1bf3099b-4363-41fe-e65c-75010e35f16a"},"cell_type":"code","source":"print(f'Test shape: {test.shape}')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_vOtJH7tK-Oa","outputId":"65ab50f4-9c68-4944-eace-d49c62b7fc8f"},"cell_type":"code","source":"#peek at a premise/hypothesis pair and their label\nprint(f\"Premise: {train['premise'].values[0]}\")\nprint(f\"Hypothesis: {train['hypothesis'].values[0]}\")\nprint(f\"Label: {train['label'].values[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wKhxVI-TK-Oc","outputId":"2492b413-3062-4fee-d667-d7fa55dd5720"},"cell_type":"code","source":"#peek at a premise/hypothesis pair and their label\nprint(f\"Premise: {train['premise'].values[1]}\")\nprint(f\"Hypothesis: {train['hypothesis'].values[1]}\")\nprint(f\"Label: {train['label'].values[1]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"arEg6IHvK-Oe","outputId":"82d07d4d-b5c2-4f5f-db15-24e8c723d1dd"},"cell_type":"code","source":"#explore the distribution of classes and languages\nfig, ax = plt.subplots(figsize = (15, 10))\n\n#for maximum aesthetics\npalette = sns.cubehelix_palette(8, start=2, rot=0, dark=0, light=.95, reverse=True)\n\ngraph1 = sns.countplot(train['language'], hue = train['label'], palette = palette)\n\n#set title\ngraph1.set_title('Distribution of Languages and Labels')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Back-Translation\n\n**In computer vision problems, there is a virtual infinitude of techniques you can use to augment your images ranging from simple techniques like randomly flipping images to blending images together with CutMix or MixUp. In natural language processing, it is not as easy to come up with similar augmentation strategies because it is hard to determine which transformations will preserve the meaning of the original words:**\n\n![](https://amitness.com/images/semantic-invariance-nlp.png)\n*Image from [@amitness](https://www.kaggle.com/amitness) on his excellent post on NLP augmentation [here](https://amitness.com/2020/05/data-augmentation-for-nlp/)*   \n\n\n**The first thought I had was to randomly replace words with their synonyms or to randomly add word synonyms to the sequence, but then I saw [this kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations) which is based on [this discussion thread](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038) and realized we can do better: we can use translation to augment our data and do try several things:**\n\n1. We can experiment and see if training our model on one language is better/worse than training on multiple languages\n2. We can change the distribution of languages in our dataset, perhaps translating sentences to low-resource languages like Swahili and Urdu\n3. We can randomly translate sentences to another language and then translate them back to the original like so:\n\n\n![](https://amitness.com/images/backtranslation-en-fr.png)\n\n*Image from [@amitness](https://www.kaggle.com/amitness) on his excellent post on NLP augmentation [here](https://amitness.com/2020/05/data-augmentation-for-nlp/)*\n\n**Please note that some of these language codes are slightly different within the `googletrans` Python API. See [here](https://py-googletrans.readthedocs.io/en/latest/) for more**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def back_translate(sequence, PROB = 1):\n    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n                 'sw', 'vi', 'es', 'el']\n    \n    #instantiate translator\n    translator = Translator()\n    \n    #store original language so we can convert back\n    org_lang = translator.detect(sequence).lang\n    \n    #randomly choose language to translate sequence to  \n    random_lang = np.random.choice([lang for lang in languages if lang is not org_lang])\n    \n    if org_lang in languages:\n        #translate to new language and back to original\n        translated = translator.translate(sequence, dest = random_lang).text\n        #translate back to original language\n        translated_back = translator.translate(translated, dest = org_lang).text\n    \n        #apply with certain probability\n        if np.random.uniform(0, 1) <= PROB:\n            output_sequence = translated_back\n        else:\n            output_sequence = sequence\n            \n    #if detected language not in our list of languages, do nothing\n    else:\n        output_sequence = sequence\n    \n    return output_sequence\n\n#check performance\nfor i in range(5):\n    output = back_translate('I genuinely have no idea what the output of this sequence of words will be')\n    print(output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I have already created augmented datasets with the above translation method in my kernel [here](https://www.kaggle.com/tuckerarrants/using-google-translate-for-nlp-augmentation/edit/run/40695539), and the datasets can be found found [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontranslationaug) and [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontwicetranslatedaug). Let's quickly compare the three separate datasets:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#offline loading of augmented datasets\ntrain_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/translation_aug_train.csv')\ntrain_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#offline loading of augmented datasets\ntrain_twice_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_train.csv')\ntrain_twice_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#offline loading of augmented datasets\ntrain_thrice_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/thrice_translation_aug_train.csv')\ntrain_thrice_aug.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wonderful! Now you could take this dataset and apply the same procedure to generate an ever more diverse set of augmetations, or you could increase the complexity of the translation by chaining together multiple languages, i.e.**\n\n> English -> French -> Russian -> .... -> English\n\n**You can experiment to see if adding samples from these back-translated training datasets gives you better performance OR you can use the back-translated test datasets for TTA**"},{"metadata":{},"cell_type":"markdown","source":"# Upsampling with Translation\n\n**Now we can use Google Translate to add additional samples for training by translating premise/hypothesis pairs to 'low-resource' languages. I have already done this [here](https://www.kaggle.com/tuckerarrants/using-google-translate-for-nlp-augmentation) so we can just import them. The following datasets are the original train dataset translated to Vietnamese, Hindi, and Bulgarian:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get CSV files\ntrain_vi = pd.read_csv(\"../input/contradictorytranslatedtrain/train_vi.csv\")\ntrain_hi = pd.read_csv(\"../input/contradictorytranslatedtrain/train_hi.csv\")\ntrain_bg = pd.read_csv(\"../input/contradictorytranslatedtrain/train_bg.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sanity check\ntrain_vi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sanity check\ntrain_hi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sanity check\ntrain_bg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Great, so if we wanted to experiment with adding these newly translated samples, we can simply add them to training. Be careful though, you do not want RoBERTa to overfit the validation data by seeing labels of the same premise/hypothesis pair in different languages, so split your data into train/validation before upsampling and remove the validation samples from your training set, if you decide to experiment with this upsampling technique**"},{"metadata":{},"cell_type":"markdown","source":"# RoBERTa\n\n**HuggingFace Transformers makes it unbelievable easy to use transformers. In fact, you don't even need to specify the transformer or tokenizer: its architecture can be guessed from the name or path of the pretrained model you specify in the `from_pretrained` method. To read more about AutoModels/Tokenizers, see [this](https://huggingface.co/transformers/model_doc/auto.html)**"},{"metadata":{"trusted":true,"id":"gqeX9fczK-Ok","outputId":"d61509f8-be18-442e-d1a7-bd1f5b080174"},"cell_type":"code","source":"#get HuggingFace transformers\n!pip install --quiet transformers\n\n#import model and Tokenizer\nfrom transformers import TFAutoModel, AutoTokenizer\n\n#get paths to TensorFlow XLM-RoBERTa base and large models\nroberta_base = \"jplu/tf-xlm-roberta-base\"\nroberta_large = 'jplu/tf-xlm-roberta-large'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Original Dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#offline load back-translated test samples\ntest_bt = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/translation_aug_test.csv')\ntest_bt_twice = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_test.csv')\ntest_bt_thrice = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/thrice_translation_aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below is a function that covers the 2 step process where we tokenize our text data with a HuggingFace object `TOKENIZER` and then convert it into a `tf.data.Dataset` object for use with TPU:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"TOKENIZER = AutoTokenizer.from_pretrained(roberta_large)\n\n#function to encode text and convert dataset to tensor dataset\ndef to_tf_dataset(dataset, max_len, repeat = False, shuffle = False, labeled = True, batch_size = BATCH_SIZE):\n    dataset_text = dataset[['premise', 'hypothesis']].values.tolist()\n    dataset_enc = TOKENIZER.batch_encode_plus(dataset_text, pad_to_max_length = True, max_length = max_len)\n    \n    if labeled:\n        tf_dataset = tf.data.Dataset.from_tensor_slices((dataset_enc['input_ids'], dataset['label']))\n    else:\n        tf_dataset = tf.data.Dataset.from_tensor_slices((dataset_enc['input_ids']))\n    \n    if repeat: tf_dataset = tf_dataset.repeat()  \n        \n    if shuffle: \n        tf_dataset = tf_dataset.shuffle(2048)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        tf_dataset = tf_dataset.with_options(opt)\n        \n    tf_dataset = tf_dataset.batch(batch_size)\n    tf_dataset = tf_dataset.prefetch(AUTO)\n    \n    return tf_dataset","execution_count":null,"outputs":[]},{"metadata":{"id":"2uAHlCN6K-Os"},"cell_type":"markdown","source":"**It is generally a good idea to use a learning rate scheduler when transfer learning. Our pretrained model already knows quite a bit, so we want to start the learning rate at 0 - if we start with a high learning rate, there is a chance we 'erase' the weights that the model already had, defeating the purpose of transfer learning. We then slowly increase the learning rate as the model adapts to the new data:** \n\n**That being said, I am still figuring out the best learning rate schedule as the current one does not seem to provide much increase in score/smoother training**"},{"metadata":{"trusted":true,"id":"mruu9foJK-Os","outputId":"c089c26a-6f60-4753-ddca-77e833fc478a"},"cell_type":"code","source":"###########################################\n#### Configuration\n###########################################\nLR_START = 1e-6\nLR_MAX = 1e-6 * 8\nLR_MIN = 1e-6\nLR_RAMPUP_EPOCHS = 2\nLR_SUSTAIN_EPOCHS = 0\nLR_DECAY = .8\n\n#stepwise schedule\ndef lrfn_step(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n    return lr\n\n\n#smoothish schedule\ndef lrfn_smooth(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback_step = tf.keras.callbacks.LearningRateScheduler(lrfn_step, verbose = True)\nlr_callback_smooth = tf.keras.callbacks.LearningRateScheduler(lrfn_smooth, verbose = True)\n\n#visualize learning rate schedule\nrng = [i for i in range(25)]\ny1 = [lrfn_step(x) for x in rng]\ny2 = [lrfn_smooth(x) for x in rng]\nfix, ax = plt.subplots(1,2, figsize = (15, 5))\nax[0].plot(rng, y1)\nax[1].plot(rng, y2)\nplt.tight_layout()\nprint(\"Learning rate schedule for step schedule: {:.3g} to {:.3g} to {:.3g}\".format(y1[0], max(y1), y1[-1]))\nprint(\"Learning rate schedule for smooth schedule: {:.3g} to {:.3g} to {:.3g}\".format(y2[0], max(y2), y2[-1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"aax9hzclK-Ou"},"cell_type":"markdown","source":"**Let's create a simple model with a RoBERTa layer connected to a `softmax` activated layer with 3 nodes to classify our premise/hypothesis pairs as 3 different labels:**"},{"metadata":{"trusted":true,"id":"pbO97U7qK-Ou"},"cell_type":"code","source":"#helper function to create our model\ndef build_model(transformer_layer, max_len, learning_rate):\n    #must use this to send to TPU cores\n    with strategy.scope():\n        #define input(s)\n        input_ids = tf.keras.Input(shape = (max_len,), dtype = tf.int32)\n        \n        #insert roberta layer\n        roberta = TFAutoModel.from_pretrained(transformer_layer)\n        roberta = roberta(input_ids)[0]\n        \n        #only need <s> token here, so we extract it now\n        out = roberta[:, 0, :]\n        \n        out = tf.keras.layers.BatchNormalization()(out)\n        \n        #add our softmax layer\n        out = tf.keras.layers.Dense(3, activation = 'softmax')(out)\n        \n        #assemble model and compile\n        model = tf.keras.Model(inputs = input_ids, outputs = out)\n        model.compile(\n                        optimizer = tf.keras.optimizers.Adam(lr = learning_rate), \n                        loss = 'sparse_categorical_crossentropy', \n                        metrics = ['accuracy'])\n        \n    return model  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\n###########################################\n#### Configuration\n###########################################\n\nLR_RATE = 5e-6\nEPOCHS = 15\nFOLDS = 4\nMAX_LEN = 85\nSTEPS_PER_EPOCH = len(train) // BATCH_SIZE\nTTA = 4\nVERBOSE = 2\n\n############################################\n#### Training\n############################################\n\npreds = np.zeros((len(test), 3))\npreds_tta = np.zeros((len(test), 3))\nskf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n\nfor fold,(train_index,val_index) in enumerate(skf.split(train, train['language'])):\n\n    #to clear TPU memory\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    #build model\n    K.clear_session()\n    model = build_model(roberta_large, max_len = MAX_LEN, learning_rate = LR_RATE)\n        \n    #save best model from each fold\n    sv = tf.keras.callbacks.ModelCheckpoint(f'fold-{fold}.h5', monitor = 'val_loss', verbose = 0,\n                        save_best_only = True, save_weights_only = True, mode = 'min')\n   \n    #get our datasets\n    train_ds = to_tf_dataset(train.loc[train_index], labeled = True, shuffle = True, repeat = True, max_len = MAX_LEN)\n    val_ds = to_tf_dataset(train.loc[val_index], labeled = True, shuffle = False, repeat = False, max_len = MAX_LEN)\n\n\n    #and go\n    print('')\n    print('#'*25); print('#### FOLD',fold+1)\n    print('Training...'); print('')\n    history = model.fit(train_ds, validation_data = val_ds, callbacks = [sv],\n                        epochs = EPOCHS, steps_per_epoch = STEPS_PER_EPOCH,\n                        verbose = VERBOSE); print('')\n\n    \n    print('Loading best model...')\n    model.load_weights(f'fold-{fold}.h5')\n    \n############################################\n#### Validation\n############################################\n    \n    #predict validation with TTA\n    print('Predicting validation with TTA...')\n    \n    #offline load pre-back-translated datasets\n    val_df = train.loc[val_index]\n    val_df_bt = train_aug.loc[val_index]\n    val_df_bt_twice = train_twice_aug.loc[val_index]\n    val_df_bt_thrice = train_thrice_aug.loc[val_index]\n    \n    #convert to tensor dataset\n    val_tta1 = to_tf_dataset(val_df, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    val_tta2 = to_tf_dataset(val_df_bt, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    val_tta3 = to_tf_dataset(val_df_bt_twice, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    val_tta4 = to_tf_dataset(val_df_bt_thrice, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    \n    #predict with augmentated validation sets\n    val_pred1 = model.predict(val_tta1, verbose = VERBOSE)\n    val_pred2 = model.predict(val_tta2, verbose = VERBOSE) \n    val_pred3 = model.predict(val_tta3, verbose = VERBOSE) \n    val_pred4 = model.predict(val_tta4, verbose = VERBOSE) \n        \n    val_preds = (val_pred1 + val_pred2 + val_pred3 + val_pred4) / TTA\n     \n    print(f\"Without TTA: {accuracy_score(val_pred1.argmax(axis = 1), val_df['label'])}\")\n    print(f\"With TTA: {accuracy_score(val_preds.argmax(axis = 1), val_df['label'])}\")\n    print('')\n    \n############################################\n#### Prediction\n############################################\n\n    #predict out of fold with TTA\n    print('Predicting OOF with TTA...')\n    \n    #convert test to tensor dataset\n    test_tta1 = to_tf_dataset(test, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    test_tta2 = to_tf_dataset(test_bt, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    test_tta3 = to_tf_dataset(test_bt_twice, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    test_tta4 = to_tf_dataset(test_bt_thrice, shuffle = False, labeled = False, repeat = False, max_len = MAX_LEN)\n    \n    #predict with augmentated validation sets\n    pred1 = model.predict(test_tta1, verbose = VERBOSE)\n    pred2 = model.predict(test_tta2, verbose = VERBOSE) \n    pred3 = model.predict(test_tta3, verbose = VERBOSE)\n    pred4 = model.predict(test_tta4, verbose = VERBOSE) \n        \n    preds_tta += (pred1 + pred2 + pred3 + pred4) / TTA / FOLDS\n    preds += pred1 / FOLDS\n\n    #so we don't hit memory limits\n    os.remove(f\"/kaggle/working/fold-{fold}.h5\")\n    del model ; z = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_TTA = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if USE_TTA:\n    submission = pd.DataFrame()\n    submission['id'] = test['id']\n    submission['prediction'] = preds_tta.argmax(axis = 1)\n\nelse:\n    submission = pd.DataFrame()\n    submission['id'] = test['id']\n    submission['prediction'] = preds.argmax(axis = 1)\n    \n#sanity check \nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)\nprint('Submission saved')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
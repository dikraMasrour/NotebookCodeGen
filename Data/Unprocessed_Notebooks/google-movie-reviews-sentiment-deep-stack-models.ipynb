{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/word2vec-nlp-tutorial/\"))\nprint(os.listdir(\"../input/movie-review/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"* First we import important packages like pandas,nltk,re,os \n* we use pandas to handle our dataset it is used to take input of test and training data then \n* we import stopwords to remove usnecessary words like is,are,names etc from the dataset we use re to keep only words\n* i will explain this in details where we use re. then we import os for setting directory\n* some worldcloud and barplot visualization "},{"metadata":{"_uuid":"fa579840ecd2215db88bdb24a4d2f2046265202d"},"cell_type":"markdown","source":"***if you dont have any of these files then you can download these files from command prompt  pip install module name\nfor pandas --- pip install pandas for nltk ---- pip install nltk then you have to download stopwords by going to python editor and import nltk then nltk.download() select all from gui or you can make custom download i suggest you to download all. Rest are inbuilt in python(excluding keras i explained thoses below) just import and enjoy.***"},{"metadata":{"_uuid":"63c4e913f7d55ab04cf228251a96fa199de4e283"},"cell_type":"markdown","source":"## First use pandas pd.read_csv() for reading these tabulated files and our basic process will be like \n* importing data \n* cleaning them \n* visualizing them \n* our stack models for deep leaning with "},{"metadata":{"trusted":true,"_uuid":"12fd8960945a69a5a2e0ca9eeed7edfecfe57465"},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"182cb20c31f72e6b217a999a5e72fffce5c44ecf"},"cell_type":"code","source":"df_train1=pd.read_csv(\"../input/movie-review/imdb_master.csv\",encoding=\"latin-1\")\ndf_train1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fe4d99d32acad6eb1169861341a62d70aebcc90"},"cell_type":"markdown","source":"## dropping unecessary columns from additional dataset and combine as one ,\n**step 1 done\nnow after this cleaning will startdown**"},{"metadata":{"trusted":true,"_uuid":"94c5be8eb27dc73ef5602b474b8a8016a44a38ea"},"cell_type":"code","source":"df_train1=df_train1.drop([\"type\",'file'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d627e7e23a3f08e53b1e9ce982c08e574ce4188a"},"cell_type":"code","source":"df_train1.rename(columns={'label':'sentiment',\n                          'Unnamed: 0':'id',\n                          'review':'review'}, \n                 inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"284d280f2031b444988ed54c86a2d1098d1e8341"},"cell_type":"code","source":"df_train1 = df_train1[df_train1.sentiment != 'unsup']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"884b0639c3a0fb7490e9ad0187a4aa384e7e2a26"},"cell_type":"code","source":"maping = {'pos': 1, 'neg': 0}\ndf_train1['sentiment'] = df_train1['sentiment'].map(maping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62c8d83d7b9080a78b0cc664253eeca410673127"},"cell_type":"code","source":"new_train=pd.concat([df_train,df_train1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ca2291a3b8a9d4d2d0e52029e3e0454e3a6a096"},"cell_type":"code","source":"df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e06aba08a708f8894c2b691b88965aeb656944e3"},"cell_type":"code","source":"new_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb7e13675b4479124c7642330e62f3111217d7b5"},"cell_type":"markdown","source":"# So for now files importing has been done \n# Step 2 begins that is data cleaning"},{"metadata":{"_uuid":"13241b65d5aee459ab29e54a74fea16fa40c844b"},"cell_type":"markdown","source":"# making function for filtering the text"},{"metadata":{"_uuid":"667a5142a21ed739f9531edeb220f5c918e7a6e9"},"cell_type":"markdown","source":"*  removing html tags \n*  re.sub(\"[^a-zA-Z]\",\" \", raw_review)\" \n* in this line we will keep all the alphabetical words which are present in the file name raw_review all special characters are replaced by a space.\n* spliting of words with normalizing it \n* taking stopwords into account \n* checking words alphanumeric or not \n* then after checking we stopwords finding and removing them \n* joinning meaningful words"},{"metadata":{"trusted":true,"_uuid":"b6f9554cee6f44fb975343c7a751868ffe1098c2"},"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef review_to_words( raw_review ):\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n    \n    # 2. Remove non-letters with regex\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                           \n    \n    # 4. Create set of stopwords\n    stops = set(stopwords.words(\"english\"))                  \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))   \n\nnew_train['review']=new_train['review'].apply(review_to_words)\ndf_test[\"review\"]=df_test[\"review\"].apply(review_to_words)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0cd280c64a2dbd9c840a2562a69b21ce28b5b53"},"cell_type":"markdown","source":"# using above function and store the filter things in array"},{"metadata":{"trusted":true,"_uuid":"9985185187475a01a65ace549cac2cf37a3d54c0"},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(new_train[\"review\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcefb2635886e91f717c414a870c936b228edeca"},"cell_type":"code","source":"# checking nullity in the data of train and test\nnew_train.isnull().sum(),df_test.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d07ca3579ecdbf8f6b5131e1883b3d21cd8516fe"},"cell_type":"markdown","source":"# importing keras files \n## tokenizer and padding is necessary so that long and short reviews must be of same length\n## what is stacking idea , first to create word2vec dictionary , word embbeding \n## apply cnn with maxpooling to find out features of neg and positive sentiment \n## apply lstm bi directional unit to for need of good memory \n\nBackground of the Techniques\nConvolution Neural Networks (CNN):\n\nCNN’s are efficient for sentence classification tasks as the convolution layers can extract features horizontally from multiple words . These characteristics are essential for classification tasks as it is tricky to find clues about class memberships especially when these clues can appear in different orders in the input.  CNN has also been used for document topic classifications where a single local phrase could aid in establishing the topic regardless of the position where it appears in the document. They found that CNN is powerful enough to find these local indicators due to the powerful combination of the convolution and pooling layers.\nLong Short-Term Memory (LSTM):\n\nAn example of LSTM’s effectiveness is its ability to capture changing sentiment in a tweet. A sentence such as “The movie was fine but not to my expectation” contains words with conflicting sentiments which is not able to be inferred accurately by a typical neural network. However, LSTM will learn that the sentiments expressed towards the end of the sentence would carry more important context compared to the words at the start.\n\nCNN — LSTM Model:\n\nThe final model architecture is . We initialized the model with Keras’ Sequential layer and added the embedding layer as the first layer. By using the embedding layer, the positive integers is turned into a dense vector of fixed size and this new representations will be passed to the CNN layer. Each filter in the CNN will detect specific features or patterns and then it will be pooled to a smaller dimension in the max-pooling layer. These features are then passed into a single LSTM layer of 100 units. Then, the LSTM outputs are then fed to a Fully Connected Layer (FCL) which was built using Keras’s Dense layer. As there are five labels to be predicted, a softmax activation function was used at the output layer.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"63fd01b6e37210eb565871b5ede1cc1f4a2d7352"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"990ecfb1fc12d223d1be3038229a3472bf2f1de1"},"cell_type":"code","source":"list_classes = [\"sentiment\"]\ny = new_train[list_classes].values\nlist_sentences_train = new_train[\"review\"]\nlist_sentences_test = df_test[\"review\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20fa4fe7c9257f5fae24f7f013b08cc922e33e3e"},"cell_type":"markdown","source":"# tokenize upto max 6000 words \n# then using keras function of preprocessing of tokenizing and padding \n"},{"metadata":{"trusted":true,"_uuid":"447280bb00b9034b45b00a5e2caf79d7b9aa026b"},"cell_type":"code","source":"max_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6803eee46ecea0d4012253ba95e8e10f5890f574"},"cell_type":"markdown","source":"# checking distribution of word length"},{"metadata":{"trusted":true,"_uuid":"251de7329c03dab7b00d8c3e737667a01f850ce2"},"cell_type":"code","source":"totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\nplt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\nplt.xlabel(\"Distribution of comment\")\nplt.ylabel(\"no of comments\")\nplt.title(\"no of comments vs no of words distribution \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dafd7ae8697d6e474e966e3f6004eff207193e23"},"cell_type":"markdown","source":"# maxlen of review is 400 words "},{"metadata":{"trusted":true,"_uuid":"e0a99b17da5ced34dc0e375fffaa3b2df75dd4b5"},"cell_type":"code","source":"maxlen = 370\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e627b8995ea0bf793644cef12013f8cbc5fe6a48"},"cell_type":"code","source":"inp = Input(shape=(maxlen, ))\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec18d958a3977e75e8592d0e5e50ee7d3f473933"},"cell_type":"code","source":"x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0299da888f1ec80bb081d67464f33f98166a67fd"},"cell_type":"code","source":"batch_size = 32\nepochs = 2\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ac8d10ec366afc8cea5d9281ee997f80c267bf"},"cell_type":"code","source":"prediction = model.predict(X_te)\ny_pred = (prediction > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865e2aa5fc1d2ef9e675050cbbf991d91e81081e"},"cell_type":"code","source":"df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f3c5114419d1b2922f07be77321348e777fdbda"},"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af0ba814cb29ec32bc972c41d6e02f3af643df15"},"cell_type":"code","source":"# ouput submission file \ndf_test = df_test[['id','sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17b0ef978a1eddf5d8d64e6756bea94dab4f71da"},"cell_type":"code","source":"df_test.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50542a12cb92b637082ac457c148cdc3a28634a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
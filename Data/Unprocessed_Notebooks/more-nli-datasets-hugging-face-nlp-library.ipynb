{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://raw.githubusercontent.com/chiapas/kaggle/master/competitions/contradictory-my-dear-watson/header.png\" width=\"1000\"></center>\n<br>\n<center><h1>Detecting contradiction and entailment in multilingual text using TPUs</h1></center>\n<br>\n\n#### Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\n#### In this notebook, we will use more NLI datasets, including\n\n* [The Stanford Natural Language Inference Corpus (SNLI)](https://nlp.stanford.edu/projects/snli/)\n* [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https://cims.nyu.edu/~sbowman/multinli/)\n* [Cross-lingual NLI Corpus (XNLI)](https://cims.nyu.edu/~sbowman/xnli/)\n\n#### We will also use Hugging Face recent library [nlp](https://huggingface.co/nlp/) to work with these datasets.\n\n## Update:\n### Since it is found that (some of) the test examples comes from these external datasets, I remove the training part in this notebook in order not to encourage using them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Import","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nimport sklearn\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\n\n# Hugging Face new library for datasets (https://huggingface.co/nlp/)\n!pip install nlp\nimport nlp\n\nimport datetime\n\nstrategy = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Competition dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n\noriginal_train = sklearn.utils.shuffle(original_train)\noriginal_train = sklearn.utils.shuffle(original_train)\n\nvalidation_ratio = 0.2\nnb_valid_examples = max(1, int(len(original_train) * validation_ratio))\n\noriginal_valid = original_train[:nb_valid_examples]\noriginal_train = original_train[nb_valid_examples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"original - training: {len(original_train)} examples\")\noriginal_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"original - validation: {len(original_valid)} examples\")\noriginal_valid.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\nprint(f\"original - test: {len(original_test)} examples\")\noriginal_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's use Hugging Face new library [nlp](https://huggingface.co/nlp/), to get more NLI datasets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Load a dataset - The Multi-Genre NLI Corpus (MNLI)\nFirst, let's load the [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https://cims.nyu.edu/~sbowman/multinli/). It contains $433000$ sentence pairs annotated with textual entailment information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli = nlp.load_dataset(path='glue', name='mnli')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### check the loaded dataset\n\nLet's look some information about the MNLI dataset. The (default) return value of [nlp.load_dataset](https://huggingface.co/nlp/package_reference/loading_methods.html#nlp.load_dataset) is a dictionary with split names as keys, usually they are `train`, `validation` and `test`, but not always. The values are [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset).\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mnli, '\\n')\n\nprint('The split names in MNLI dataset:')\nfor k in mnli:\n    print('   ', k)\n    \n# Get the datasets\nprint(\"\\nmnli['train'] is \", type(mnli['train']))\n\nmnli['train']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### look inside 'nlp.arrow_dataset.Dataset'\n\nIn order to get the number of examples in a dataset, for example, `mnli['train']`, you can do\n```\n    mnli['train'].num_rows\n```\n\n\nYou can iterate a [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset) object like:\n```\n    for elt in mnli['train']:\n        ...\n```\nEach step, you get an example (which is a dictionary containing features - in a general sense).\n\nYou can also access the content of a [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset) object by specifying a feature name . For example, the training dataset in `mnli` has `premise`, `hypothesis`, `label` and `idx` as features.\n\nYou can either specify a feature name first (you get a list) followed by a slice, like\n```\n    # You get a `list` first, then slice it\n    mnli['train']['premise'][:3]\n```\nor use slice notation first to get a dictionary (which represents a sliced dataset) followed by a feature name.\n```\n    # You get a `dictionary` (of lists) first, then a list\n    mnli['train'][:3]['premise']\n```\n\nThe results will be the same.\n\nIn order to get the name of the classes, you can do\n\n```\nmnli['train'].features['label'].names\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's use what we learned to check some training examples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)\nprint('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)\nprint('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\\n')\n\nprint('The class names in mnli dataset:', mnli['train'].features['label'].names)\nprint('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\\n')\n\nfor elt in mnli['train']:\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', mnli['train'].features['label'].names[elt['label']])\n    print('idx', elt['idx'])\n    print('-' * 80)\n    \n    if elt['idx'] >= 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset, described in [this competition data page](https://www.kaggle.com/c/contradictory-my-dear-watson/data):\n\n> label: the classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Convert MNLI to pandas.DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to a dataframe and view\nmnli_train_df = pd.DataFrame(mnli['train'])\nmnli_valid_1_df = pd.DataFrame(mnli['validation_matched'])\nmnli_valid_2_df = pd.DataFrame(mnli['validation_mismatched'])\n\nmnli_train_df = mnli_train_df[['premise', 'hypothesis', 'label']]\nmnli_valid_1_df = mnli_valid_1_df[['premise', 'hypothesis', 'label']]\nmnli_valid_2_df = mnli_valid_2_df[['premise', 'hypothesis', 'label']]\n\nmnli_train_df['lang_abv'] = 'en'\nmnli_valid_1_df['lang_abv'] = 'en'\nmnli_valid_2_df['lang_abv'] = 'en'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_valid_1_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_valid_2_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load more extra datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The Stanford Natural Language Inference Corpus (SNLI)\n\nFirst, let's load the [The Stanford Natural Language Inference Corpus (SNLI)](https://nlp.stanford.edu/projects/snli/). It contains $570000$ sentence pairs annotated with textual entailment information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"snli = nlp.load_dataset(path='snli')\n\nprint('The number of training examples in snli dataset:', snli['train'].num_rows)\nprint('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\\n')\n\nprint('The class names in snli dataset:', snli['train'].features['label'].names)\nprint('The feature names in snli dataset:', list(snli['train'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(snli['train']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', snli['train'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset.\n\nIn [SNLI](https://nlp.stanford.edu/projects/snli/), we have the same premise with different hypotheses/labels. With a first try, I got `nan` as the training loss value. So I won't use this dataset in the current notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Convert SNLI to pandas.DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to a dataframe and view\nsnli_train_df = pd.DataFrame(snli['train'])\nsnli_valid_df = pd.DataFrame(snli['validation'])\n\nsnli_train_df = snli_train_df[['premise', 'hypothesis', 'label']]\nsnli_valid_df = snli_valid_df[['premise', 'hypothesis', 'label']]\n\nsnli_train_df['lang_abv'] = 'en'\nsnli_valid_df['lang_abv'] = 'en'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snli_train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snli_valid_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Cross-Lingual NLI Corpus (XNLI)\n\nThe [MNLI](https://cims.nyu.edu/~sbowman/multinli/) and [SNLI](https://nlp.stanford.edu/projects/snli/) contain only english sentences. Let's load the [Cross-lingual NLI Corpus (XNLI)](https://cims.nyu.edu/~sbowman/xnli/) dataset. It contains only validation and test dataset, not training examples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xnli = nlp.load_dataset(path='xnli')\n\nprint('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\\n')\n\nprint('The class names in xnli dataset:', xnli['validation'].features['label'].names)\nprint('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(xnli['validation']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', xnli['validation'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 3:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The class names are still\n```\n    ['entailment', 'neutral', 'contradiction'],\n```\nhowever, the features `premise` and `hypothesis` are no longer `string` but `dictionary` which contain sentences in different language! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Convert XNLI to pandas.DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to a dataframe and view\nbuffer = {\n    'premise': [],\n    'hypothesis': [],\n    'label': [],\n    'lang_abv': []\n}\n\n\nfor x in xnli['validation']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buffer['premise'].append(premise)\n        buffer['hypothesis'].append(hypothesis)\n        buffer['label'].append(label)\n        buffer['lang_abv'].append(lang)\n        \n# convert to a dataframe and view\nxnli_valid_df = pd.DataFrame(buffer)\nxnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xnli_valid_df.head(15 * 3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
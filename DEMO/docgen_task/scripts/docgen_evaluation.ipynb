{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the generated documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:\\\\Users\\\\dmasrour\\\\Documents\\\\CodeDoc_Generation\\\\Data_backup\\\\DocGen_Unprocessed_Notebooks\\\\kaggle-courses-master\\\\intro_to_machine_learning\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + '03-your-first-machine-learning-model.ipynb', encoding=\"utf8\") as jsonfile:\n",
    "    original_nb = json.load(jsonfile)\n",
    "original_nb = pd.json_normalize(original_nb['cells'])[['cell_type', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + '03-your-first-machine-learning-model.ipynbPLBART_documented.ipynb', encoding=\"utf8\") as jsonfile:\n",
    "    docgen_nb = json.load(jsonfile)\n",
    "docgen_nb = pd.json_normalize(docgen_nb['cells'])[['cell_type', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  cell_type                                             source\n",
       " 0  markdown  [**[Introduction to Machine Learning Home Page...\n",
       " 1  markdown  [## Recap\\n, So far, you have loaded your data...\n",
       " 2      code  [# Code you have previously used to load data\\...\n",
       " 3  markdown  [# Exercises\\n, \\n, ## Step 1: Specify Predict...\n",
       " 4      code  [# print the list of columns in the dataset to...,\n",
       " (22, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_nb.head(), original_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  cell_type                                             source\n",
       " 0  markdown      [Loads the data from the previous used code.]\n",
       " 1      code  [# Code you have previously used to load data\\...\n",
       " 2  markdown  [Print the list of columns in the dataset to t...\n",
       " 3      code  [# print the list of columns in the dataset to...\n",
       " 4  markdown                  [Check the value of thealePrice.],\n",
       " (16, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docgen_nb.head(), docgen_nb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT score (takes too long without GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed BLEU-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(ref, gen):\n",
    "    ''' \n",
    "    calculate pair wise bleu score. uses nltk implementation\n",
    "    Args:\n",
    "        references : a list of reference sentences \n",
    "        candidates : a list of candidate(generated) sentences\n",
    "    Returns:\n",
    "        bleu score(float)\n",
    "    '''\n",
    "    ref_bleu = []\n",
    "    gen_bleu = []\n",
    "    for l in gen:\n",
    "        gen_bleu.append(l.split())\n",
    "    for i,l in enumerate(ref):\n",
    "        ref_bleu.append([l.split()])\n",
    "    cc = SmoothingFunction()\n",
    "    score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(1, 0, 0, 0), smoothing_function=cc.method4)\n",
    "    return score_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14715177646857694"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ['Create the list of features below the given feature names.']\n",
    "references = [\"After you've created that list of features, use it to create the DataFrame that you'll use to fit the model.\"]\n",
    "bleu(references, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT vectors cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"Select the target variable, which corresponds to the sales price. Save this to a new variable called `y`. You'll need to print a list of the columns to find the name of the column you need.\"\n",
    "toks_ref = tokenizer.tokenize(ref)\n",
    "\n",
    "tokens = [tokenizer.cls_token] + toks_ref + [tokenizer.sep_token]\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ref_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = 'hello'\n",
    "toks_gen = tokenizer.tokenize(gen)\n",
    "\n",
    "tokens = [tokenizer.cls_token] + toks_gen + [tokenizer.sep_token]\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "gen_embeddings = model(torch.tensor(tokens_ids)[None,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensor into np array\n",
    "tensor_np_ref = ref_embeddings.cpu().detach().numpy()\n",
    "tensor_np_gen = gen_embeddings.cpu().detach().numpy()\n",
    "# average of embeddings of the tokens in the sequence\n",
    "avg_ref = np.mean(tensor_np_ref[0], axis=0)\n",
    "avg_gen = np.mean(tensor_np_gen[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8215266466140747"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = 1 - cosine(avg_ref, avg_gen)\n",
    "cos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49ab456e11cde720218fba409a85456f40f210cf294d5c8f56d5f4fb69af5c6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
